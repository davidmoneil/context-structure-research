---
created: 2026-01-20T17:00
updated: 2026-01-24T10:44
tags:
  - project/tweenagers
  - depth/deep
  - domain/security
  - depth/standard
  - domain/infrastructure
---


**The simplest effective architecture combines a credential vault pattern with BullMQ orchestration and an OAuth broker—achievable in roughly 1,000 lines of TypeScript** by extracting patterns from Nango's open-source codebase. For most home lab scenarios, existing tools like **Windmill** or **Trigger.dev** provide 80% of the functionality with far less effort than building from scratch. The key decision hinges on whether you need per-user OAuth tokens (build custom) or primarily system-to-system integrations (use existing tools).

This analysis covers lightweight alternatives, DIY architecture patterns, extractable components from Nango, and a decision framework for your specific use case with n8n, Authentik, and PostgreSQL already in your stack.

## Existing tools offer surprising capability for zero cost

**Windmill emerges as the strongest contender** for a ready-made solution. This YC-backed, AGPLv3-licensed platform runs on Docker Compose with ~2GB RAM, provides native TypeScript support, and includes a built-in "Resources" system for encrypted credential storage. Scripts automatically generate UIs, APIs, and cron jobs—eliminating much infrastructure work. Windmill positions itself as a "simplified Temporal with autogenerated UIs" and benchmarks as the fastest self-hostable workflow engine.

**Trigger.dev offers a code-first alternative** under Apache 2.0 licensing. Designed specifically for TypeScript developers, it provides long-running tasks without timeouts, automatic retries, built-in integrations with Slack/Stripe/OpenAI, and real-time observability. Self-hosting requires Kubernetes via Helm or runs on Railway for $5-15/month.

| Tool | OAuth/Credential Mgmt | Push Data Back | TypeScript Native | RAM Required |
|------|----------------------|----------------|-------------------|--------------|
| **Windmill** | ✅ Built-in resources | ✅ Yes | ✅ Yes | ~2GB |
| **Trigger.dev** | ✅ Built-in integrations | ✅ Yes | ✅ Yes | ~2GB |
| **Temporal.io** | ❌ Build yourself | ✅ Yes | ✅ Excellent SDK | 2-4GB |
| **n8n** (you have this) | ⚠️ Manual OAuth setup | ✅ Yes | ❌ JavaScript only | ~2GB |
| **Airbyte** | ✅ Declarative OAuth | ❌ ETL only (pull) | ❌ Python/Java | **8GB+ min** |

**Airbyte fails this use case**—it's designed for ETL (source → data warehouse), requires 4+ CPUs and 8GB RAM minimum, and cannot push data back to APIs. Pipedream cannot be self-hosted despite source-available code. Your existing n8n installation could handle many integration scenarios, though its OAuth setup requires manual client creation in each provider's console.

## Nango's architecture reveals extractable patterns

Analyzing Nango's monorepo reveals a remarkably clean architecture built on PostgreSQL, Express, and the `simple-oauth2` npm package. The core insight: **most complexity serves enterprise multi-tenancy and horizontal scaling—features unnecessary for a home lab**.

**Provider definitions use elegant YAML configuration:**

```yaml
github:
  display_name: GitHub
  auth_mode: OAUTH2
  authorization_url: https://github.com/login/oauth/authorize
  token_url: https://github.com/login/oauth/access_token
  proxy:
    base_url: https://api.github.com
  connection_config:
    subdomain:
      type: string
      pattern: '^[a-z0-9_-]+$'
```

This declarative approach means adding a new integration requires ~20 lines of YAML rather than custom code. Nango supports OAuth 1.0a, OAuth 2.0, API keys, and Basic auth through this single format.

**Credential encryption uses AES-256-GCM** with a Base64-encoded 256-bit key stored in an environment variable (`NANGO_ENCRYPTION_KEY`). The implementation is straightforward Node.js crypto—no external vault required for basic security. Tokens, secret keys, and app secrets are encrypted at rest.

**Token refresh follows a dual strategy**: on-demand refresh when fetching connections (checking if expired), plus proactive background refresh every 24 hours to prevent token revocation from unused connections. A configurable buffer (default 15 minutes) triggers refresh before actual expiry. Redis locks prevent race conditions during concurrent refresh attempts.

**The job orchestration migrated from Temporal to a Postgres-based queue** using `FOR UPDATE SKIP LOCKED` for concurrent access—demonstrating that PostgreSQL alone handles scheduling without Redis for smaller deployments.

### Components worth extracting versus skipping

| Essential (replicate) | Nice-to-have (simplify) | Enterprise (skip) |
|-----------------------|-------------------------|-------------------|
| providers.yaml format | OAuth 1.0a support | Multi-tenant isolation |
| OAuth 2.0 flow with simple-oauth2 | Pagination helpers in proxy | Horizontal scaling |
| AES-256-GCM encryption | Multi-environment (dev/prod) | ElasticSearch logging |
| Token refresh mechanism | Custom TypeScript sync scripts | Records sync storage |
| Postgres credential storage | Webhook routing per provider | Redis caching |
| Request proxy with auth injection | | Orchestrator microservices |

A functional clone covering OAuth2, encrypted token storage, automatic refresh, and proxy requests could be built in **~1,000 lines of TypeScript over 2-3 days**.

## DIY architecture for a minimal integration hub

If building from scratch, the architecture splits into five core services that can run as a single Node.js process or separate containers:

**The credential vault uses envelope encryption in PostgreSQL.** A Key Encrypting Key (KEK) from environment variables encrypts per-credential Data Encryption Keys (DEKs), which encrypt the actual tokens. This enables key rotation without re-encrypting all credentials.

```sql
CREATE TABLE connections (
  id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
  provider_id VARCHAR(50) NOT NULL,
  user_id VARCHAR(100) NOT NULL,
  encrypted_dek BYTEA NOT NULL,
  encrypted_payload BYTEA NOT NULL,
  iv BYTEA NOT NULL,
  auth_tag BYTEA NOT NULL,
  token_expires_at TIMESTAMPTZ,
  created_at TIMESTAMPTZ DEFAULT NOW()
);
```

**The OAuth broker centralizes callback handling** at a single endpoint (`/oauth/callback/:provider`) that looks up provider configuration from YAML, exchanges authorization codes for tokens, encrypts credentials, and stores them. The `simple-oauth2` library handles OAuth complexity; your code manages provider registry and storage.

**Job orchestration works well with BullMQ or pg-boss.** BullMQ requires Redis but provides sophisticated features like FlowProducer for parent-child job dependencies, exponential backoff with jitter, and rate limiting per queue. pg-boss runs entirely on PostgreSQL—eliminating Redis—with atomic transactions that commit jobs alongside business data. For under 50 integrations, pg-boss likely suffices.

```typescript
// pg-boss: PostgreSQL-native queue (no Redis)
const boss = new PgBoss({ connectionString: process.env.DATABASE_URL });
await boss.schedule('token-refresh-check', '*/5 * * * *');
```

**Webhook ingestion follows queue-first architecture**: verify signature immediately, enqueue payload, return 200 within milliseconds, then process asynchronously. The "fetch before process" pattern treats webhooks as notifications—fetching fresh data from the API rather than trusting potentially stale or out-of-order webhook payloads.

**Self-documenting APIs come free with tsoa**, which generates OpenAPI specs from TypeScript controller decorators. Swagger UI then serves interactive documentation automatically.

### Recommended project structure

```
integration-hub/
├── apps/
│   ├── api/              # Express + tsoa controllers
│   ├── worker/           # BullMQ/pg-boss job processors  
│   └── webhook-gateway/  # Fast signature verification + enqueue
├── packages/
│   ├── connectors/       # Per-provider: auth, sync, webhooks
│   │   ├── base/         # Connector interface
│   │   ├── github/
│   │   └── stripe/
│   ├── database/         # Prisma/Drizzle schema + migrations
│   └── crypto/           # Envelope encryption utilities
└── docker/
    └── docker-compose.yml
```

The connector plugin architecture uses a registry pattern—each provider exports a `Connector` interface with methods for `getAuthUrl`, `exchangeCode`, `refreshToken`, `sync`, and `verifyWebhook`. Adding integrations becomes adding a new folder with ~100 lines following the template.

## API gateways and Authentik have specific limitations

**API gateways handle system-level credentials but not per-user tokens.** Kong's Upstream OAuth plugin (enterprise-only), Tyk's upstream authentication, and KrakenD's client credentials all inject configured OAuth tokens into upstream requests. They excel at service-to-service auth where one set of credentials serves all traffic.

For home lab use, **KrakenD stands out**: stateless single binary, no database required, configuration-as-code, HashiCorp Vault integration, and Linux Foundation backing. But none of these gateways manage per-user OAuth tokens for scenarios like "each user connects their own GitHub account."

**Authentik cannot serve as an API credential vault.** Authentik Sources are designed for user enrollment—importing identity from external IdPs—not for storing access tokens to reuse in API calls. The proxy provider injects user identity headers (`X-authentik-username`, `X-authentik-jwt`) to backends but doesn't inject third-party API credentials. Authentik excels at what you're already using it for (user SSO, access control) but requires a separate system for API credential management.

The recommended pattern: **Authentik for user identity → API Gateway (optional) for routing/rate limiting → Custom credential vault for stored tokens → BullMQ workers for sync jobs**.

## MCP servers expose integrations to AI agents elegantly

The Model Context Protocol SDK (`@modelcontextprotocol/sdk`) enables exposing your integration hub to Claude and other AI assistants. Tools represent callable functions; Resources provide read-only data for context.

```typescript
server.registerTool(
  "query_crm_contacts",
  {
    description: "Search contacts in connected CRM",
    inputSchema: {
      integration: z.enum(["salesforce", "hubspot"]),
      query: z.string(),
    },
  },
  async ({ integration, query }) => {
    const credentials = await getCredentials(integration);
    const results = await queryAPI(integration, 'contacts', query, credentials);
    return { content: [{ type: "text", text: JSON.stringify(results) }] };
  }
);
```

**Dynamic tool generation** based on database-stored integration definitions means adding a new provider automatically exposes it to AI assistants without code changes. The MCP server queries your integration registry and generates tools at startup.

Configure Claude Desktop with your server:

```json
{
  "mcpServers": {
    "integration-hub": {
      "command": "node",
      "args": ["/path/to/build/index.js"],
      "env": { "DATABASE_URL": "postgresql://..." }
    }
  }
}
```

## Decision framework for your specific situation

Given your context—TypeScript/Node.js developer, Docker home lab, n8n already running, Authentik for user auth, PostgreSQL/Supabase, under 50 integrations, $0 budget—here's the decision tree:

**If your integrations are primarily system-to-system** (your services calling APIs with your credentials):

→ **Use Windmill.** Deploy via Docker Compose, store credentials in Resources, build flows for sync jobs. Your existing n8n handles simple automations; Windmill handles TypeScript-native integration logic with better credential management.

**If you need per-user OAuth tokens** (each user connects their own accounts):

→ **Build a minimal custom hub** extracting Nango patterns. The 1,000-line TypeScript implementation covers: providers.yaml config, OAuth2 flows with `simple-oauth2`, AES-256-GCM credential encryption, token refresh with pg-boss, and a proxy endpoint injecting auth headers.

**If you want maximum control with enterprise patterns:**

→ **Temporal.io + custom credential layer.** Temporal provides bulletproof durable execution; you build OAuth and storage on top. Higher operational complexity but infinite flexibility.

**Quick wins with your existing stack:**

Your n8n installation already handles many integration patterns—consider whether you truly need a separate hub or just better organization of n8n workflows. pg-boss eliminates Redis from the architecture. Authentik continues handling user identity; don't try to extend it for API credentials.

### Minimum viable integration hub

For the simplest architecture providing credential vault + OAuth flows + data sync + push capabilities:

1. **Single Express/Fastify server** with tsoa for API documentation
2. **PostgreSQL tables**: `providers` (YAML configs), `connections` (encrypted credentials), `sync_state` (cursors)
3. **pg-boss** for job scheduling (token refresh, sync jobs, webhook processing)
4. **simple-oauth2** for OAuth flow management
5. **Node.js crypto** for AES-256-GCM envelope encryption
6. **Optional MCP server** exposing integrations to AI assistants

This runs in a single Docker container with PostgreSQL, requires ~500MB RAM, and handles your 50-integration scale comfortably.

## Conclusion: the middle ground exists

The "middle ground between full Nango and completely DIY" is either **Windmill** (if you want a UI and managed credentials) or **extracting Nango's core patterns** (if you want pure TypeScript control). Nango's complexity serves enterprise multi-tenancy you don't need; their core innovation—YAML provider definitions + simple-oauth2 + encrypted Postgres storage—replicates in a weekend.

Your existing n8n + Authentik + PostgreSQL stack gets you surprisingly far. The incremental value of a dedicated integration hub comes when you need: centralized credential management across multiple MVPs, reliable token refresh for OAuth APIs, or programmatic access to integration data (especially via MCP for AI agents). For under 50 integrations, prioritize simplicity—pg-boss over Redis, single-container deployment, YAML configuration over database-driven providers.