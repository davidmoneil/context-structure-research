---
type: claude-knowledge
source: aiprojects
source_path: ".claude/agent-output/results/deep-research/2026-01-30_ollama-amd-strix-halo-gfx1151.md"
source_category: "deep-research"
synced: 2026-02-17
title: "Deep Research: Ollama on AMD Ryzen AI MAX+ 395 (Strix Halo / gfx1151)"
tags:
  - claude-knowledge
  - deep-research
---

# Deep Research: Ollama on AMD Ryzen AI MAX+ 395 (Strix Halo / gfx1151)

**Research Date**: 2026-01-30
**Confidence Level**: High (multiple sources corroborated)
**Sources Consulted**: 25+

## Executive Summary

The AMD Ryzen AI MAX+ 395 (Strix Halo) with gfx1151 GPU architecture represents cutting-edge hardware for local LLM inference, capable of running models up to 128B parameters thanks to its 128GB unified memory architecture. However, software support is still maturing. ROCm support arrived with version 6.4.1 but remains partially unofficial. Ollama gained gfx1151 support via PR #9773, and the community fork `ollama-for-amd` provides extended support. For optimal performance, users should consider either **Vulkan backend** (simpler setup, good for short conversations) or **ROCm HIP with rocWMMA** (better for long context). The XDNA 2 NPU cannot currently run LLMs via Ollama/llama.cpp but AMD's FastFlowLM provides an alternative NPU runtime.

---

## Key Findings

### 1. ROCm Compatibility

| ROCm Version | Status | Notes |
|--------------|--------|-------|
| 6.3 | Not supported | No gfx1151 support |
| 6.4.1 | Working (unofficial) | First working version for Strix Halo |
| 6.4.4 | Officially supported | AMD's official supported version |
| 7.0 | Working | Requires Linux 6.14+ kernel |
| 7.1+ | Unstable | Known GPU hang issues with concurrent AI + video encoding |

**Recommended**: ROCm 6.4.4 (official) or ROCm 7.0 with TheRock nightlies for best gfx1151 support.

**Known Issues**:
- GPU hangs when running AI workloads + video encoding simultaneously (ROCm 7.1)
- VRAM visibility limited to ~15.5GB on older kernels (fixed in kernel 6.16.9+)
- llama.cpp/Llamafile segfaults on some ROCm versions
- Blender HIP backend crashes in some scenes

**Sources**: [ROCm GitHub Issue #5339](https://github.com/ROCm/ROCm/issues/5339), [Phoronix ROCm Benchmarks](https://www.phoronix.com/review/amd-strix-halo-rocm-benchmarks), [ROCm GitHub Issue #5665](https://github.com/ROCm/ROCm/issues/5665)

---

### 2. Ollama AMD Support

**Official Status**: gfx1151 support added via [PR #9773](https://github.com/ollama/ollama/pull/9773)

**Community Fork**: [ollama-for-amd](https://github.com/likelovewant/ollama-for-amd) provides extended ROCm support including gfx1151 with ROCm 6.4.2.

**Supported GPU List** (from ollama-for-amd):
- gfx906, gfx1010, gfx1012, gfx1030-1036
- gfx1100, gfx1101, gfx1103
- **gfx1150, gfx1151, gfx1152, gfx1153** (Strix series)
- gfx1200, gfx1201

**Workarounds Required**:
1. Use kernel 6.16.9+ to access full unified memory
2. May need `HSA_OVERRIDE_GFX_VERSION=11.0.0` for some ROCm components

**Sources**: [Ollama Issue #9553](https://github.com/ollama/ollama/issues/9553), [ollama-for-amd Wiki](https://github.com/likelovewant/ollama-for-amd/wiki)

---

### 3. Optimal Configuration

#### Environment Variables

```bash
# Core Ollama settings
export OLLAMA_FLASH_ATTENTION=1
export OLLAMA_NUM_PARALLEL=2  # Adjust based on model size
export OLLAMA_KEEP_ALIVE=5m
export OLLAMA_MAX_LOADED_MODELS=1  # For large models

# AMD ROCm settings
export HSA_OVERRIDE_GFX_VERSION=11.0.0  # If needed for compatibility
export ROCR_VISIBLE_DEVICES=0
export HIP_VISIBLE_DEVICES=0
export ROCBLAS_USE_HIPBLASLT=1

# Performance tuning
export TORCH_ROCM_AOTRITON_ENABLE_EXPERIMENTAL=1
export OMP_NUM_THREADS=16  # Match your P-core count
```

#### Unified Memory Configuration (Linux)

**BIOS Settings**:
1. Set "UMA Frame Buffer Size" to 512MB (display framebuffer only)
2. Disable IOMMU
3. The actual compute memory is handled by GTT

**Kernel Parameters** (Ubuntu/Fedora):
```bash
# For up to 108GB GPU allocation
sudo grubby --update-kernel=ALL --args='amdttm.pages_limit=27648000'
sudo grubby --update-kernel=ALL --args='amdttm.page_pool_size=27648000'
```

**Note**: While BIOS typically limits to 96GB, Linux can allocate ~108GB before errors occur.

**Sources**: [Jeff Geerling VRAM Guide](https://www.jeffgeerling.com/blog/2025/increasing-vram-allocation-on-amd-ai-apus-under-linux), [Framework Community](https://community.frame.work/t/ryzen-ai-max-395-how-should-i-configure-the-gpu-vram-from-the-bios-settings/76753), [GitHub Setup Guide](https://github.com/Gygeek/Framework-strix-halo-llm-setup)

---

### 4. Parallel Inference Configuration

| Setting | Value | Notes |
|---------|-------|-------|
| `OLLAMA_NUM_PARALLEL` | 1-4 | Higher = more concurrent requests, more memory |
| `OLLAMA_MAX_LOADED_MODELS` | 1-2 | For large models, keep at 1 |
| `OLLAMA_MAX_QUEUE` | 4 | Maximum queued requests |

**Calculation**: `Total RAM / Model Size * 0.8 = Max Parallel`

For a 70B Q4 model (~40GB): `96GB / 40GB * 0.8 â‰ˆ 2 parallel`

**Sources**: [Ollama GPU Docs](https://docs.ollama.com/gpu), [Collabnix Guide](https://collabnix.com/ollama-gpu-acceleration-the-ultimate-nvidia-cuda-and-amd-rocm-configuration-guide-for-production-ai-deployment/)

---

### 5. LM Studio vs Ollama Comparison

| Feature | Ollama | LM Studio |
|---------|--------|-----------|
| **AMD ROCm Support** | Partial (gfx1151 via PR/fork) | Limited official, custom ROCm possible |
| **Vulkan Support** | Via llama.cpp backend | Built-in |
| **Performance** | ~20% faster inference | GUI overhead |
| **Best For** | Developers, production | Beginners, GUI users |
| **gfx1151 Status** | Supported via community | Requires custom ROCm libs |
| **Memory Efficiency** | Better | More overhead |

**Recommendation**: For Strix Halo, **Ollama with Vulkan backend** or **raw llama.cpp** provides the best experience. LM Studio is viable with Vulkan but ROCm support is less mature.

**Sources**: [LM Studio Bug #1269](https://github.com/lmstudio-ai/lmstudio-bug-tracker/issues/1269), [Glukhov Comparison](https://www.glukhov.org/post/2025/11/hosting-llms-ollama-localai-jan-lmstudio-vllm-comparison/)

---

### 6. NPU Utilization (XDNA 2)

**Current Status**: The XDNA 2 NPU (50 TOPS) **cannot** be used for LLM inference via Ollama or llama.cpp.

**Alternative Solution**: [FastFlowLM](https://rits.shanghai.nyu.edu/ai/fastflowlm-running-llms-on-amd-ryzen-ai-npus-with-ease/)
- Lightweight runtime (~14 MB)
- Supports up to 256K context length
- OpenAI-compatible API
- Integrated into AMD's Lemonade Server (Oct 2025)
- Works on XDNA2 NPUs (Strix, Strix Halo, Kraken)

**AMD Official Tools**:
- AMD Quark for model quantization (SmoothQuant, GPTQ, Quarot)
- AMD Lemonade for serving/benchmarking on CPU, GPU, and NPU
- Ryzen AI Software stack for ONNX models on NPU

**Sources**: [AMD Ryzen AI Software](https://www.amd.com/en/developer/resources/ryzen-ai-software.html), [Ollama NPU Issue #5186](https://github.com/ollama/ollama/issues/5186), [llama.cpp XDNA Issue #1499](https://github.com/ggml-org/llama.cpp/issues/1499)

---

### 7. Performance Expectations

#### Token Generation Speed (tok/s)

| Model | Backend | pp (prompt) | tg (generation) |
|-------|---------|-------------|-----------------|
| Phi-3.5 | ROCm | ~1940 | 61 |
| Llama 7B Q4_0 | ROCm+rocWMMA | 998 | 50 |
| Llama 7B | Vulkan | - | 54 |
| gemma3:1b | Ollama | 540 | 157 |
| gpt-oss:20b | Ollama | 1939 | 23.8 |
| gpt-oss:120b | Ollama | - | 14.8 |
| Qwen 3 8B | Vulkan | Better | Slightly better |
| Llama 4 Scout 109B (17B active) | ROCm | - | ~4x faster than 70B |

**Key Insight**: MoE (Mixture of Experts) models like Llama 4 Scout perform exceptionally well on Strix Halo due to the large unified memory + limited bandwidth architecture.

**Sources**: [AMD Official Blog](https://www.amd.com/en/developer/resources/technical-articles/2025/amd-ryzen-ai-max-395--a-leap-forward-in-generative-ai-performanc.html), [GMKTec Benchmarks](https://nishtahir.com/gmktec-evo-x2-ryzen-ai-max-395-benchmarks/), [Framework Community Tests](https://community.frame.work/t/amd-strix-halo-ryzen-ai-max-395-gpu-llm-performance-tests/72521)

---

### 8. Fine-Tuning Support

**Capability**: Full, LoRA, and QLoRA fine-tuning supported on Strix Halo.

**Model Capacity**:
- Full fine-tuning: Up to 12B parameters
- LoRA/QLoRA: Up to 20-30B parameters

**Tested Models**:
- Gemma-3
- Qwen-3
- GPT-OSS-20B

**Resources**:
- [amd-strix-halo-llm-finetuning](https://github.com/kyuz0/amd-strix-halo-llm-finetuning) - Fedora toolbox with ROCm 7 nightly
- [amd-strix-halo-toolboxes](https://github.com/kyuz0/amd-strix-halo-toolboxes)

**Requirements**:
```python
# For Gemma/Qwen training
model = AutoModel.from_pretrained(
    model_name,
    attn_implementation="eager"  # FlashAttention 2 is for inference only
)
```

**Environment Variables**:
```bash
export HSA_OVERRIDE_GFX_VERSION=11.0.0
```

**llama.cpp flags**:
```bash
llama-server -fa 1 --no-mmap  # Always use these on Strix Halo
```

**Sources**: [Framework Fine-tuning Guide](https://community.frame.work/t/finetuning-llms-on-strix-halo-full-lora-and-qlora-on-gemma-3-qwen-3-and-gpt-oss-20b/76986), [Dev.to Gemma Finetuning](https://dev.to/ankk98/observations-from-finetuning-gemma-model-on-strix-halo-fedora-43-2cdo)

---

## Best Practices Summary

### For Inference (Ollama/llama.cpp)

1. **Backend Choice**:
   - Short conversations (1-shot): **Vulkan + Flash Attention**
   - Long context: **ROCm HIP + rocWMMA + Flash Attention**

2. **Build llama.cpp** (if compiling):
   ```bash
   cmake -DGGML_HIP_ROCWMMA_FATTN=ON ..
   ```

3. **Runtime flags**:
   ```bash
   llama-server -fa 1 --no-mmap
   ```

4. **Kernel**: Use 6.16.9+ for full memory visibility

5. **ROCm Version**: 6.4.4 (stable) or 7.0 with TheRock nightlies (cutting edge)

### For Fine-Tuning

1. Use `attn_implementation="eager"` for training
2. Full fine-tuning limited to ~12B params
3. LoRA/QLoRA enables ~20-30B params
4. Use ROCm 7 nightlies from TheRock

---

## Conflicting Information

| Topic | Source A | Source B | Resolution |
|-------|----------|----------|------------|
| VRAM Limit | 96GB (BIOS max) | 108GB (Linux limit) | Both true - BIOS cap vs kernel cap |
| rocWMMA | Faster (Aug 2025) | Slower (ROCm 7.0.2+) | Changed over time - check current guidance |
| Vulkan vs HIP | Vulkan faster | HIP faster for long context | Both valid - depends on workload |

---

## Knowledge Gaps

1. **NPU + GPU Hybrid Inference**: No current solution for splitting inference between NPU and iGPU
2. **Production Stability**: Long-term stability data for 24/7 inference workloads limited
3. **Windows Performance**: Most benchmarks are Linux; Windows ROCm support is worse
4. **Docker Support**: Limited documentation on containerized Ollama with ROCm on Strix Halo

---

## Recommendations

### Immediate Setup

1. Install Ubuntu 24.04 LTS with kernel 6.16.9+
2. Install ROCm 6.4.4 (stable) or use TheRock nightlies
3. Configure GTT memory for ~96-108GB
4. Use Ollama with community fork or upstream (post-PR #9773)
5. Start with Vulkan backend for simplicity

### For Production

1. Test extensively with your specific workloads
2. Avoid simultaneous AI + video encoding (known crash bug)
3. Monitor for GPU hangs with `dmesg -w`
4. Consider llama.cpp directly for maximum control

### For Fine-Tuning

1. Use TheRock nightly ROCm builds
2. Follow the amd-strix-halo-llm-finetuning repo
3. Start with LoRA on 8B models before scaling up

---

## Related Topics for Follow-up Research

- ROCm 7.2+ stability improvements for gfx1151
- FastFlowLM production readiness
- vLLM AMD support for multi-GPU scenarios
- Hybrid CPU+GPU inference optimization

---

## Sources

1. [Phoronix ROCm Benchmarks](https://www.phoronix.com/review/amd-strix-halo-rocm-benchmarks) - High credibility
2. [ROCm GitHub Issues](https://github.com/ROCm/ROCm/issues/5339) - Primary source
3. [Ollama GitHub PR #9773](https://github.com/ollama/ollama/pull/9773) - Primary source
4. [ollama-for-amd](https://github.com/likelovewant/ollama-for-amd) - Community fork
5. [Framework Community Tests](https://community.frame.work/t/amd-strix-halo-ryzen-ai-max-395-gpu-llm-performance-tests/72521) - High credibility
6. [Jeff Geerling VRAM Guide](https://www.jeffgeerling.com/blog/2025/increasing-vram-allocation-on-amd-ai-apus-under-linux) - High credibility
7. [AMD Official Blog](https://www.amd.com/en/developer/resources/technical-articles/2025/amd-ryzen-ai-max-395--a-leap-forward-in-generative-ai-performanc.html) - Primary source
8. [llm-tracker.info Strix Halo](https://llm-tracker.info/AMD-Strix-Halo-(Ryzen-AI-Max+-395)-GPU-Performance) - Aggregated data
9. [llama.cpp HIP Discussion](https://github.com/ggml-org/llama.cpp/discussions/15021) - Technical details
10. [Strix Halo Wiki](https://strixhalo.wiki/AI/llamacpp-with-ROCm) - Community documentation
11. [GMKTec Benchmarks](https://nishtahir.com/gmktec-evo-x2-ryzen-ai-max-395-benchmarks/) - Real-world testing
12. [AMD Ryzen AI Software](https://www.amd.com/en/developer/resources/ryzen-ai-software.html) - NPU documentation
13. [amd-strix-halo-llm-finetuning](https://github.com/kyuz0/amd-strix-halo-llm-finetuning) - Fine-tuning guide
