---
type: claude-knowledge
source: aiprojects
source_path: ".claude/agent-output/results/deep-research/2026-02-17_claude-code-knowledge-fast-interfaces.md"
source_category: "deep-research"
synced: 2026-02-17
title: "Deep Research: Exposing Claude Code Knowledge Bases Through Faster Interfaces"
tags:
  - claude-knowledge
  - deep-research
---

# Deep Research: Exposing Claude Code Knowledge Bases Through Faster Interfaces

**Research Date**: 2026-02-17
**Confidence Level**: High
**Sources Consulted**: 22

## Executive Summary

The core problem is bridging a rich Claude Code environment (279 markdown files, 4MB of context, MCP servers, custom tools) to interactive interfaces like Claude Desktop App, where headless `claude -p` is too slow (1-50+ minutes). Six approaches were researched, ranging from simple API passthrough to full RAG pipelines.

The **recommended approach is a hybrid architecture**: a custom MCP server (Approach 2) for knowledge queries combined with n8n direct API calls (Approach 3) for interactive Q&A, with prompt caching to keep latency under 5 seconds. Heavy tasks that require tool use, Docker access, or code execution continue to route through headless Claude Code. This gives you sub-5-second interactive knowledge queries while preserving the full Claude Code capability for complex work.

The RAG approach (Approach 4) adds value as an enhancement layer but is not necessary as the primary solution -- your knowledge base at ~4MB fits comfortably within Claude's context window (200K tokens), meaning direct injection with prompt caching is both simpler and more accurate than semantic search.

## Key Findings

### 1. n8n AI Agent Node with Claude API

**How it works**: n8n has a native AI Agent node that uses LangChain under the hood. You configure it with an Anthropic Chat Model sub-node, set a system prompt, and attach tool nodes. The system prompt is a text field in the node configuration -- you can dynamically inject file contents using preceding nodes.

**Architecture**:
```
[Webhook/MCP Trigger] --> [Read File nodes to load context] --> [Merge context into system prompt] --> [AI Agent (Anthropic)] --> [Response]
```

**Context injection pattern**:
- Use n8n's "Read File" or "Execute Command" nodes to read `.claude/context/*.md` files
- Concatenate them into a system prompt string
- Pass to the AI Agent node's system message field
- The Anthropic Chat Model sub-node supports `claude-sonnet-4-6`, `claude-haiku-4-5`, etc.

**Expected latency**: 3-15 seconds depending on model and context size. The n8n overhead adds ~500ms. First call with new context takes longer (cache creation); subsequent calls within 5 minutes benefit from Anthropic's prompt caching.

**What is preserved**:
- Knowledge/context content (injected as system prompt)
- Conversational ability
- Can attach n8n tools (HTTP requests, database queries, etc.)

**What is lost**:
- Claude Code's native tool use (Bash, Edit, Write, LSP)
- MCP server integrations
- CLAUDE.md automatic loading
- Hooks system
- Session continuity

**Implementation complexity**: LOW-MEDIUM. n8n's visual workflow builder makes this straightforward. Main work is building the context-loading pipeline.

**n8n MCP pipeline compatibility**: YES -- the n8n instance can expose this workflow via MCP Server Trigger, which Claude Desktop connects to via `mcp-remote` proxy (bridges SSE to stdio).

**Cost considerations**: Each request sends the full system prompt. With prompt caching, repeated queries within 5 minutes cost only 10% of base input token price for the cached portion. For Haiku 4.5, that is $0.10/MTok for cached reads.

---

### 2. Custom MCP Server Exposing Knowledge (RECOMMENDED)

**How it works**: Build a dedicated MCP server in Node.js or Python that exposes your `.claude/context/` and `knowledge/` directories as MCP resources and search tools. Claude Desktop connects to it natively -- the knowledge appears as tools Claude can call.

**Architecture**:
```
Claude Desktop --> MCP protocol (stdio) --> Custom Knowledge MCP Server --> Local filesystem
```

**What the MCP server exposes**:

*Resources* (read-only data Claude can reference):
- `context://index` -- The `_index.md` file listing all context
- `context://{path}` -- Any specific context file
- `knowledge://{path}` -- Any knowledge base file

*Tools* (functions Claude can call):
- `search_context(query)` -- Full-text search across all context files
- `list_context_files(category?)` -- List available context by category
- `read_context_file(path)` -- Read a specific context file
- `search_knowledge(query)` -- Search knowledge base
- `get_system_state()` -- Read current system state, priorities, etc.
- `query_beads(filter)` -- Query task database via `bd list`

**Implementation approach** (using `@modelcontextprotocol/sdk`):

The server would use `McpServer` from the SDK with `StdioServerTransport`. Key tools would use `ripgrep` for fast full-text search across all markdown files, with results returned as text content blocks. Each tool validates paths against allowed directories (CONTEXT_DIR and KNOWLEDGE_DIR) to prevent directory traversal.

For the Beads integration, the `query_beads` tool would shell out to `bd list` with appropriate flags and return the output.

**Claude Desktop configuration** (`~/.claude/mcp.json` or Claude Desktop settings):
```json
{
  "mcpServers": {
    "aiprojects-knowledge": {
      "command": "node",
      "args": ["/home/davidmoneil/Code/aiprojects-knowledge-mcp/dist/index.js"],
      "env": {
        "CONTEXT_DIR": "/home/davidmoneil/AIProjects/.claude/context",
        "KNOWLEDGE_DIR": "/home/davidmoneil/AIProjects/knowledge"
      }
    }
  }
}
```

**Expected latency**: 50-200ms for file reads, 200-500ms for searches. This is the fastest approach for knowledge queries.

**What is preserved**:
- All knowledge/context content (on-demand, not pre-loaded)
- Claude Desktop's native conversation experience
- Can be combined with other MCP servers
- Search capability across the knowledge base

**What is lost**:
- Claude Code's tool execution (Bash, Edit, Docker)
- Hooks, session management
- The behavioral instructions from CLAUDE.md (though you could expose CLAUDE.md as a promptable resource)

**Implementation complexity**: MEDIUM. The MCP SDK is well-documented. Main work is deciding what tools/resources to expose and handling edge cases. Estimated 4-8 hours for a solid v1.

**n8n MCP pipeline compatibility**: YES -- if n8n already connects to Claude Desktop via MCP, this server runs alongside. Claude Desktop connects to both n8n MCP and knowledge MCP simultaneously.

**Existing projects to reference**:
- [mcp-local-rag](https://github.com/shinpr/mcp-local-rag) -- Local RAG with MCP, ~1.2s queries
- [claude-knowledge-base-mcp](https://github.com/sitechfromgeorgia/claude-knowledge-base-mcp) -- Persistent knowledge for Claude Desktop
- [knowledge-mcp](https://lobehub.com/mcp/olafgeibig-knowledge-mcp) -- Knowledge domain bridge for AI assistants
- [basic-memory](https://playbooks.com/mcp/basicmachines-co/basic-memory) -- Markdown-based semantic graph MCP

---

### 3. n8n Workflow with Direct Claude API Calls

**How it works**: Skip both Claude Code and the AI Agent node entirely. Use n8n's HTTP Request node to call `https://api.anthropic.com/v1/messages` directly, with system prompt assembled from filesystem reads.

**Architecture**:
```
[MCP Trigger / Webhook]
  --> [Read Binary Files: load context/*.md]
  --> [Function Node: assemble system prompt with cache_control]
  --> [HTTP Request: POST to Anthropic Messages API]
  --> [Parse response]
  --> [Return to MCP client]
```

**n8n workflow structure**:
1. **Trigger**: MCP Server Trigger (Claude Desktop calls this) or Webhook (for other clients)
2. **Context Loader**: Execute Command node runs file reads, or uses Read File nodes for each context file
3. **Prompt Builder** (Function node): Assembles the API payload. The system array contains two text blocks -- the first with instructions, the second with concatenated context files marked with `cache_control: { type: "ephemeral" }`.
4. **HTTP Request**: POST to `https://api.anthropic.com/v1/messages` with headers `x-api-key`, `anthropic-version: 2023-06-01`, `content-type: application/json`. Model set to `claude-haiku-4-5` for fast/cheap knowledge queries.

**Prompt caching impact**: With ~4MB of context (~roughly 30-40K tokens), the first call creates a cache (costs 1.25x base input). Subsequent calls within 5 minutes read from cache (costs 0.1x base input). With Haiku 4.5:
- First call: ~$0.05 (cache write)
- Subsequent calls: ~$0.004 (cache read) -- 90% cheaper
- Latency reduction: up to 85% on cache hits

**Expected latency**:
- First call (cache creation): 3-8 seconds
- Cached calls: 1-3 seconds
- n8n overhead: ~200-500ms

**What is preserved**:
- All knowledge content (injected as system prompt)
- Full control over model selection, parameters
- Prompt caching for cost/latency optimization
- Streaming support possible

**What is lost**:
- Tool use (unless you implement tool calling loop in n8n)
- MCP server integrations
- Session state, memory
- Multi-turn conversation (unless you manage history in n8n)

**Implementation complexity**: LOW-MEDIUM. Simpler than the AI Agent approach because you have direct control. The main complexity is the context-loading pipeline and managing conversation history if needed.

**n8n MCP pipeline compatibility**: YES -- expose via MCP Server Trigger. Claude Desktop calls this as a tool.

---

### 4. RAG Approach (Vector Database + Semantic Search)

**How it works**: Index all context/knowledge files into a vector database using embeddings. Provide semantic search via MCP tools. When Claude Desktop asks a question, relevant chunks are retrieved and injected into the prompt.

**Architecture**:
```
[Indexing Pipeline]
Context files --> Chunk --> Embed (local model) --> Store in ChromaDB/LanceDB

[Query Pipeline]
Claude Desktop --> MCP Tool: search_knowledge(query)
  --> Embed query --> Vector similarity search --> Return top-K chunks
  --> Claude uses chunks as context for response
```

**Embedding options (local, no API costs)**:
- `all-MiniLM-L6-v2` via Transformers.js (~90MB, runs on CPU, 384-dim vectors)
- Ollama embedding models (if already running)
- SentenceTransformers Python library

**Vector database options**:
- **LanceDB**: File-based, no server process needed, perfect for single-user. Used by mcp-local-rag.
- **ChromaDB**: Lightweight, Python-native, good for local use. Can run embedded or as server.
- **Qdrant**: More production-grade, Docker deployment.

**Existing MCP-RAG servers**:
- [mcp-local-rag](https://github.com/shinpr/mcp-local-rag): LanceDB + Transformers.js, 6 MCP tools, ~1.2s queries
- [rag-cli](https://github.com/ItMeDiaTech/rag-cli): ChromaDB + all-MiniLM-L6-v2, sub-5s queries
- [mcp-rag-with-chromadb](https://lobehub.com/mcp/cyprianfusi-mcp-rag-with-chromadb): Multi-format support, LangChain powered

**Chunking strategy for your files**:
- Context files (`.claude/context/`): Chunk by section headers (## headings)
- Knowledge docs: 500-token chunks with 50-token overlap
- Small files (<1000 tokens): Keep as single chunks

**Expected latency**:
- Indexing: One-time, ~30-60 seconds for 279 files
- Query: 200ms-1.5s depending on implementation
- Re-indexing on file changes: Can be triggered by filesystem watcher

**What is preserved**:
- Semantic understanding of knowledge content
- Efficient retrieval of relevant sections
- Works well for large knowledge bases (would scale to 10,000+ files)

**What is lost**:
- Full context awareness (only retrieves chunks, not full documents)
- May miss connections between documents
- Requires indexing pipeline maintenance
- Claude Code tool use, hooks, session management

**Implementation complexity**: MEDIUM-HIGH. Requires embedding model setup, vector DB, chunking pipeline, re-indexing mechanism, and MCP server wrapper.

**When RAG adds value over direct injection**:
- Knowledge base exceeds context window (~200K tokens / ~150K words)
- Need semantic search ("find docs about Docker networking" vs grep for "docker")
- Files change frequently and you want incremental updates

**When RAG is overkill**:
- Your current setup: ~4MB / 279 files fits in context window
- Direct search (grep/ripgrep) works well for keyword matching
- Maintenance overhead not justified for the scale

**n8n MCP pipeline compatibility**: YES -- expose RAG search as MCP tools.

**Recommendation**: Start with Approach 2 (direct file search MCP) and add RAG as an enhancement layer later if you find search quality insufficient.

---

### 5. Hybrid Approach (RECOMMENDED ARCHITECTURE)

**How it works**: Route queries based on complexity. Light knowledge queries go through fast direct API or MCP; heavy tasks that need tool execution route to headless Claude Code.

**Architecture**:
```
Claude Desktop App
  |-- [MCP: aiprojects-knowledge] --> Fast knowledge queries (50-500ms)
  |     Tools: search_context, read_context, list_files, get_state
  |
  |-- [MCP: n8n-knowledge-agent] --> AI-powered Q&A with context (1-5s)
  |     n8n workflow: load context --> Claude API (Haiku) --> response
  |     Uses prompt caching for repeated queries
  |
  +-- [MCP: n8n-heavy-tasks] --> Complex operations (1-50+ min)
        n8n workflow: trigger headless claude -p with full context
        For: Docker operations, code changes, multi-step tasks
```

**Routing logic** (implemented in Claude Desktop or n8n):

| Query Type | Route | Latency | Example |
|-----------|-------|---------|---------|
| "What port does Grafana use?" | Knowledge MCP (direct file read) | <500ms | Read inventory.md |
| "What is the status of my Docker services?" | Knowledge MCP (Beads query) | <1s | Run `bd list` |
| "Explain the orchestration pattern" | n8n AI Agent (Haiku + cached context) | 1-3s | API call with context |
| "Help me debug the Caddy config" | n8n AI Agent (Sonnet + context) | 3-8s | API call with context |
| "Rebuild the monitoring stack" | Headless Claude Code | 5-50+ min | Full tool access |

**Implementation plan**:

Phase 1 (Day 1): Custom Knowledge MCP Server
- Build the MCP server with search/read tools
- Configure in Claude Desktop
- Immediate benefit: fast file lookups

Phase 2 (Day 2-3): n8n Direct API Workflow
- Build n8n workflow with context loading + Anthropic API
- Enable prompt caching
- Expose via MCP Server Trigger
- Benefit: AI-powered Q&A with full context

Phase 3 (Optional): RAG Enhancement
- Add vector indexing for semantic search
- Either as tools in the Knowledge MCP or as a separate MCP
- Benefit: "fuzzy" search when grep is not enough

Phase 4 (Optional): Heavy Task Router
- n8n workflow that spawns headless Claude Code for complex tasks
- Async pattern: returns "task started" immediately, notifies on completion
- Benefit: complete capability parity for complex work

**Expected latency**:
- Knowledge lookups: <500ms
- AI Q&A with context: 1-5s (with prompt caching)
- Complex tasks: Same as current (1-50+ min) but no longer blocking

**What is preserved**: Everything, through appropriate routing.

**Implementation complexity**: MEDIUM overall. Phase 1-2 cover 90% of use cases with moderate effort. Phases 3-4 are optional enhancements.

**n8n MCP pipeline compatibility**: YES -- this IS the n8n MCP pipeline, enhanced.

---

### 6. OpenWebUI Integration

**How it works**: OpenWebUI already supports RAG with knowledge bases, can connect to Anthropic via community functions/pipelines, and has a web chat interface. You could mirror your Claude Code context into OpenWebUI's knowledge system.

**Architecture**:
```
OpenWebUI (already running)
  |-- Anthropic Pipeline (community function)
  |     - Claude Haiku/Sonnet via API
  |     - Supports system prompts, caching
  |
  |-- Knowledge Base (built-in RAG)
  |     - Upload .claude/context/ as knowledge collection
  |     - ChromaDB (default) or 8 other vector DB options
  |     - Top-K retrieval with configurable chunk sizes
  |
  +-- Models (Ollama)
        - Can use local models for cost-free queries
        - Good for simple lookups, not for complex reasoning
```

**Setup steps**:
1. Install Anthropic function from OpenWebUI community: `openwebui.com/f/captresolve/anthropic_claude_model_access`
2. Configure API key in function settings
3. Create Knowledge Base: Workspace > Knowledge > Create
4. Upload context files: Support `.md` format natively, can use "Sync Directory" feature
5. Assign knowledge base to a model/conversation

**RAG configuration** (Settings > Documents):
- Chunk size: 1000 (recommended for markdown docs)
- Chunk overlap: 200
- Top-K: 10 (increase for broader context)
- Embedding model: Can use Ollama's `nomic-embed-text` or default SentenceTransformer

**Directory Sync** (key feature):
OpenWebUI supports syncing a directory of documents. You could point it at `/home/davidmoneil/AIProjects/.claude/context/` and it would auto-ingest new/changed files.

**Expected latency**: 2-8 seconds for RAG-enhanced queries. The UI is web-based, adding ~100ms.

**What is preserved**:
- Knowledge content (via RAG retrieval)
- Chat interface with history
- Can use Claude or local models
- Multi-user support (if needed)

**What is lost**:
- No MCP integration (OpenWebUI does not support MCP as a client)
- Not connected to Claude Desktop pipeline
- RAG retrieval is chunked, not full-context
- No tool execution capability
- Different conversation experience than Claude Desktop

**Implementation complexity**: LOW. Mostly configuration, no code needed.

**n8n MCP pipeline compatibility**: NO -- OpenWebUI is a separate interface, not connected to the Claude Desktop MCP pipeline. However, you could use n8n as a bridge (OpenWebUI webhook -> n8n -> MCP tools).

**Verdict**: Good as a supplementary interface (especially for mobile/browser access) but does not solve the core problem of fast knowledge queries FROM Claude Desktop.

---

## Detailed Analysis

### Prompt Caching: The Key Enabler

Anthropic's prompt caching is the most important technology for this use case. Key facts:

- **Cache lifetime**: 5 minutes (default), extendable to 1 hour at 2x cost
- **Cost reduction**: Cache reads cost 10% of base input price
- **Latency reduction**: Up to 85% on cache hits
- **Minimum cacheable size**: 1024 tokens (Sonnet/Opus 4.1+), 4096 tokens (Opus 4.5+, Haiku 4.5)
- **Breakpoints**: Up to 4 per request, allowing independent caching of tools, system prompt, context, and conversation history

For your setup (~30-40K tokens of context), prompt caching means:
- First query: Normal latency + 25% cost premium for cache write
- Queries 2-N within 5 min: ~85% faster, ~90% cheaper
- If you query every few minutes, the cache stays warm perpetually

**Implementation**: Place your stable context (CLAUDE.md, system docs) in the first cache breakpoint. Place dynamic context (session state, recent tasks) in the second. This way the large static portion stays cached even when dynamic content changes.

### Your Knowledge Base Scale Analysis

| Metric | Value | Implication |
|--------|-------|-------------|
| Context files | 204 | Manageable for direct injection |
| Knowledge files | 75 | Fits in context window |
| Total size | ~4MB | ~30-40K tokens, well within 200K limit |
| File format | All Markdown | Easy to parse and inject |

At this scale, RAG is optional. Direct injection with prompt caching is simpler, more accurate, and lower maintenance. RAG becomes valuable at 10x this size.

### n8n MCP Server Architecture

Your existing n8n MCP integration uses SSE (Server-Sent Events). Claude Desktop uses stdio. The bridge:

```
Claude Desktop --> mcp-remote (stdio-to-SSE proxy) --> n8n MCP Server (SSE)
```

The `mcp-remote` npm package handles this translation. Configuration in Claude Desktop:
```json
{
  "mcpServers": {
    "n8n-knowledge": {
      "command": "npx",
      "args": ["-y", "mcp-remote", "https://your-n8n-url/mcp/endpoint"]
    }
  }
}
```

---

## Best Practices

1. **Start with the Knowledge MCP Server** (Approach 2) -- it is the fastest path to sub-second knowledge queries from Claude Desktop
2. **Add n8n AI Agent** (Approach 1 or 3) for questions that need reasoning, not just file lookups
3. **Use Haiku 4.5 for knowledge queries** -- fast, cheap ($1/MTok input, $0.10/MTok cached), good enough for Q&A
4. **Use Sonnet for complex reasoning** -- when you need analysis or synthesis across multiple documents
5. **Keep headless Claude Code for tool-heavy tasks** -- do not try to replicate Bash/Docker/Edit capabilities
6. **Use prompt caching aggressively** -- cache your system prompt and static context, keep the cache warm
7. **Build incrementally** -- do not try to build all 6 approaches at once

## Common Pitfalls

1. **Trying to replicate Claude Code entirely** -- You cannot replicate the full tool suite. Focus on the 80% use case (knowledge queries) and route the 20% (tool-heavy tasks) to Claude Code.
2. **Over-engineering RAG for small knowledge bases** -- At 4MB, direct injection is simpler and more accurate.
3. **Ignoring prompt caching** -- Without it, every API call re-processes 30-40K tokens. With it, subsequent calls are 90% cheaper and 85% faster.
4. **Using Opus for knowledge queries** -- Expensive and slow for simple lookups. Haiku 4.5 is 5x cheaper and 3x faster.
5. **Not exposing the context index** -- Make `_index.md` a resource so Claude Desktop can discover what context is available.

## Conflicting Information

- **OpenWebUI Anthropic support**: Community functions exist but quality varies. Some are well-maintained (captresolve's function), others are outdated. The official OpenWebUI team has not built native Anthropic support as of early 2026.
- **n8n MCP stability**: The MCP Server Trigger is production-ready for n8n Cloud but has known issues with reverse proxies and webhook replicas in self-hosted setups.
- **RAG chunk quality**: Sources disagree on optimal chunk sizes. Anthropic recommends contextual embeddings (adding surrounding context to chunks before embedding), which reportedly reduces retrieval failure by 35%, but this is more complex to implement.

## Knowledge Gaps

- **Claude Desktop MCP resource consumption**: How much context overhead do multiple MCP servers create? The documentation mentions MCP Tool Search activating at >10% context consumption, but real-world numbers for 3-4 servers are not documented.
- **Prompt caching with MCP tool results**: Whether tool result content counts toward cache breakpoints when returned from MCP servers has not been explicitly documented.
- **n8n prompt caching support**: The n8n AI Agent node may not support Anthropic's `cache_control` parameter natively. The direct HTTP Request approach gives you full control.

## Recommendations

### Immediate (This Week)

1. **Build the Knowledge MCP Server** (Approach 2)
   - Node.js with `@modelcontextprotocol/sdk`
   - Tools: `search_context`, `read_context`, `list_files`, `get_system_state`
   - Register in Claude Desktop config
   - Estimated time: 4-6 hours

2. **Build n8n Direct API Workflow** (Approach 3)
   - MCP Server Trigger --> Context Loader --> Anthropic API (Haiku 4.5)
   - Enable prompt caching on system prompt
   - Estimated time: 2-4 hours

### Short-term (Next 2 Weeks)

3. **Add heavy-task routing** via n8n
   - Workflow that triggers `claude -p` for complex operations
   - Async pattern with notification on completion
   - Estimated time: 4-8 hours

### Optional (When Needed)

4. **RAG enhancement** for semantic search
   - Consider `mcp-local-rag` as a starting point
   - Or build custom with LanceDB + Transformers.js
   - Only if grep-based search proves insufficient

5. **OpenWebUI knowledge sync** for browser/mobile access
   - Directory sync to auto-ingest context files
   - Low effort, supplementary interface

## Sources

1. [n8n AI Agent Integration](https://n8n.io/integrations/agent/) - Official n8n AI Agent documentation
2. [n8n Anthropic Integration](https://n8n.io/integrations/anthropic/) - Anthropic node configuration
3. [n8n MCP Server Documentation](https://docs.n8n.io/advanced-ai/accessing-n8n-mcp-server/) - Instance-level MCP access
4. [n8n MCP Server Trigger Node](https://docs.n8n.io/integrations/builtin/core-nodes/n8n-nodes-langchain.mcptrigger/) - Workflow-level MCP exposure
5. [MCP Resources Specification](https://modelcontextprotocol.io/specification/2025-06-18/server/resources) - Official MCP resource protocol
6. [MCP TypeScript SDK](https://github.com/modelcontextprotocol/typescript-sdk) - Official SDK for building MCP servers
7. [Build MCP Server Tutorial](https://modelcontextprotocol.io/docs/develop/build-server) - Official build guide
8. [FreeCodeCamp MCP Server Guide](https://www.freecodecamp.org/news/how-to-build-a-custom-mcp-server-with-typescript-a-handbook-for-developers/) - Comprehensive TypeScript tutorial
9. [Anthropic Prompt Caching Docs](https://platform.claude.com/docs/en/build-with-claude/prompt-caching) - Official prompt caching documentation
10. [Anthropic Messages API](https://docs.anthropic.com/en/api/messages) - API reference
11. [claude-knowledge-base-mcp](https://github.com/sitechfromgeorgia/claude-knowledge-base-mcp) - Knowledge management MCP for Claude Desktop
12. [mcp-local-rag](https://github.com/shinpr/mcp-local-rag) - Local RAG MCP server (LanceDB + Transformers.js)
13. [rag-cli](https://github.com/ItMeDiaTech/rag-cli) - ChromaDB RAG plugin for Claude Code
14. [knowledge-mcp](https://lobehub.com/mcp/olafgeibig-knowledge-mcp) - Knowledge domain bridge MCP
15. [Basic Memory MCP](https://playbooks.com/mcp/basicmachines-co/basic-memory) - Markdown-based semantic graph
16. [OpenWebUI RAG Tutorial](https://docs.openwebui.com/tutorials/tips/rag-tutorial/) - Official RAG documentation
17. [OpenWebUI Anthropic Function](https://openwebui.com/f/captresolve/anthropic_claude_model_access) - Community Claude integration
18. [Claude Desktop MCP Connection](https://community.n8n.io/t/connect-claude-desktop-app-to-mcp-server-trigger-node/118238) - SSE-to-stdio bridge
19. [Best MCP Servers for Knowledge Bases 2026](https://desktopcommander.app/blog/2026/02/06/best-mcp-servers-for-knowledge-bases-in-2026/) - Current landscape overview
20. [n8n MCP Production Guide](https://agence-scroll.com/en/blog/mcp-n8n-connect-claude-to-your-workflows-and-go-into-production-without-surprises) - Production deployment guide
21. [Anthropic Prompt Caching Announcement](https://www.anthropic.com/news/prompt-caching) - Feature overview and pricing
22. [Build MCP Server in Node.js](https://oneuptime.com/blog/post/2025-12-17-build-mcp-server-nodejs/view) - Step-by-step Node.js guide

## Related Topics

- MCP server deployment and management (systemd, Docker)
- Anthropic Batch API for bulk knowledge processing
- Claude Code custom slash commands that call MCP tools
- n8n credential management for Anthropic API keys
- Filesystem watcher for auto-reindexing on context file changes
- Multi-model routing (Haiku for simple, Sonnet for complex, Opus for analysis)
