---
type: claude-knowledge
source: aiprojects
source_path: ".claude/agent-output/results/deep-research/2026-02-14_context-forge-llm-orchestration.md"
source_category: "deep-research"
synced: 2026-02-17
title: "Deep Research: Context Forge -- LLM Context Orchestration State of the Art"
tags:
  - claude-knowledge
  - deep-research
---

# Deep Research: Context Forge -- LLM Context Orchestration State of the Art

**Research Date**: 2026-02-14
**Confidence Level**: High (core compression/evaluation topics) / Medium (statistical methodology specifics)
**Sources Consulted**: 28+

---

## Executive Summary

Context Forge's hypothesis -- that a small local LLM can preprocess/compress/restructure context before sending it to a target LLM to improve quality and reduce tokens -- is well-supported by existing research but occupies a specific niche that differs from both standard RAG and pure prompt compression. The field has matured rapidly since 2023, with Microsoft's LLMLingua series demonstrating 20x compression with minimal loss, RECOMP achieving 94% compression at the retrieval level, and CompactPrompt showing that compression can actually *improve* output quality on some tasks.

The key insight from the literature is that **the preprocessing approach (using a small model to restructure context for a large model) is an underexplored sweet spot between prompt compression and RAG**. Most existing work either compresses at the token level (LLMLingua) or retrieves and ranks (RAG). Context Forge's approach of having a small model *intelligently restructure* content before sending to a large model is architecturally similar to RECOMP's abstractive compressor and FrugalGPT's cascade pattern, but with a distinct emphasis on context quality engineering rather than just size reduction.

For evaluation, the LLM-as-Judge literature is mature but comes with significant caveats: position bias, verbosity bias, and self-preference bias require active mitigation. Claude as judge is a reasonable choice, but calibration against a human gold standard (30-50 examples) is essential. Statistical significance requires at minimum 5 paired runs per configuration using bootstrap + permutation tests, with ~200 calibration examples for reliable confidence intervals.

---

## 1. Existing Work in Context Compression/Optimization

### The Landscape (NAACL 2025 Survey Taxonomy)

The [NAACL 2025 Prompt Compression Survey](https://aclanthology.org/2025.naacl-long.368/) (Li et al., Cambridge) categorizes all approaches into two branches:

**Hard Prompt Methods** (modify the text itself):
- **Filtering**: SelectiveContext, LLMLingua, LongLLMLingua, LLMLingua-2, AdaComp, TACO-RL, PCRL
- **Paraphrasing**: Nano-Capsulator, CompAct, FAVICOMP

**Soft Prompt Methods** (compress into learned embeddings):
- **Decoder-only**: GIST, AutoCompressor, CC
- **Encoder-decoder**: ICAE, 500xCompressor, COCOM, LLoCO, xRAG

### Key Systems and What Works

#### LLMLingua Series (Microsoft, EMNLP'23 / ACL'24)
- [GitHub](https://github.com/microsoft/LLMLingua) | [Paper](https://arxiv.org/abs/2310.05736) | [LLMLingua-2](https://llmlingua.com/llmlingua2.html)
- **LLMLingua**: Uses a small aligned LM to compute token-level perplexity; removes low-information tokens. Achieves **up to 20x compression** with minimal performance loss. Integrated into LangChain and LlamaIndex.
- **LongLLMLingua**: Designed for long-context scenarios. Achieves **21.4% performance boost with 4x fewer tokens** on NaturalQuestions, 94% cost reduction on LooGLE benchmark.
- **LLMLingua-2**: Pivoted from perplexity-based pruning to **token classification** using XLM-RoBERTa fine-tuned on GPT-4 distillation data. **3-6x faster** than original, better on out-of-domain data, 95-98% accuracy retention.
- **Relevance to Context Forge**: LLMLingua-2's approach of using a small model (XLM-RoBERTa) trained on large model outputs is directly analogous to Context Forge's architecture.

#### Selective Context (Li et al., EMNLP'23)
- [Paper](https://aclanthology.org/2023.emnlp-main.391/)
- Uses a small LM to compute self-information per token; prunes low-information tokens.
- **50% context reduction**, 36% memory savings, 32% latency improvement, minor BERTScore/faithfulness drop.
- **Limitation**: Grammar breaks at aggressive compression rates.

#### RECOMP (Xu et al., ICLR'24)
- [Paper](https://arxiv.org/abs/2310.04408) | [GitHub](https://github.com/carriex/recomp)
- Two compressors: **extractive** (selects key sentences via contrastive learning) and **abstractive** (generates summaries distilled from GPT-3/GPT-4).
- Achieves **94% compression** (6% retention) with minimal loss on language modeling and QA.
- **Key feature**: Outputs empty string when documents are irrelevant (selective augmentation).
- **Most relevant to Context Forge**: The abstractive compressor is essentially a small model restructuring content for a larger model -- exactly the Context Forge pattern.

#### CompactPrompt (Oct 2025)
- [Paper](https://arxiv.org/abs/2510.18043)
- End-to-end pipeline combining hard prompt pruning + n-gram abbreviation + numerical quantization.
- **60% token reduction** with **<5% accuracy drop** across Claude 3.5 Sonnet, GPT-4.1-Mini, GPT-4-Omni, Llama-3.3-70B.
- **Critical finding**: Compression actually *improved* quality for Claude (+6-10 points on financial QA) and GPT-4.1-Mini (+5 points), suggesting compressed context can reduce noise.

#### DisComp (NAACL Findings 2025)
- [Paper](https://aclanthology.org/2025.findings-naacl.58/)
- Two-stage framework: task-agnostic compression followed by task-aware optimization.
- Combines compression with prompt optimization.

### What Failed or Has Limitations

1. **Aggressive token pruning (>50%)**: Produces ungrammatical text that degrades model comprehension. [Characterizing Prompt Compression](https://arxiv.org/html/2407.08892v1) showed significant accuracy degradation.
2. **Query-agnostic abstractive summarization**: Weaker summarization models omit critical information. Query-aware methods outperform by 10-15 points on complex tasks.
3. **Soft prompt methods**: Require model-specific fine-tuning, not transferable across architectures, and add inference overhead. Not suitable for API-based targets like Claude.
4. **Information-dense content** (code, math, structured data): Only achieves 2-3x compression vs 10-20x for natural language.
5. **Join query accuracy in SQL**: Dropped from 0.63 to 0.37 at 4.29x compression (task-specific cliff).

### Compression Ratio Guidelines (Practical)

| Content Type | Safe Compression | Aggressive | Cliff Point |
|-------------|-----------------|------------|-------------|
| Natural language prose | 5-10x | 10-20x | ~20x |
| Dense technical docs | 3-5x | 5-8x | ~10x |
| Code/structured data | 2-3x | 3-5x | ~5x |
| Math/formal logic | 1.5-2x | 2-3x | ~3x |
| Financial tables | 2-2.5x | 3x | ~4x |

---

## 2. LLM-as-Judge Evaluation Best Practices

### Comprehensive Survey Sources
- [A Survey on LLM-as-a-Judge (Nov 2024)](https://arxiv.org/abs/2411.15594) -- 200+ paper survey
- [How to Correctly Report LLM-as-a-Judge Evaluations](https://arxiv.org/html/2511.21140v1) -- Statistical methodology
- [Justice or Prejudice? Quantifying Biases](https://llm-judge-bias.github.io/)
- [Self-Preference Bias](https://arxiv.org/html/2410.21819v2)

### Known Biases and Severity

| Bias | Description | Severity | Mitigation |
|------|------------|----------|------------|
| **Position bias** | Favors responses in certain positions (pairwise) | High (>10% accuracy shift from order swap) | Run twice with swapped order, average scores |
| **Verbosity/length bias** | Prefers longer, more formal responses | High (artifact of RLHF training) | Normalize for length; use pointwise scoring with length controls |
| **Self-preference** | Higher scores to outputs with lower perplexity to the judge model | Medium | Use a different model family as judge vs generator |
| **Concreteness bias** | Prefers concrete over abstract content | Medium | Include in rubric that both are valid |
| **Knowledge bias** | Favors responses aligned with training data | Medium | Ground truth reference in scoring rubric |

### Scoring Format Recommendations

**For Context Forge specifically (comparing configurations):**

1. **Pointwise scoring** (not pairwise) is recommended because:
   - You're comparing same-input across many configurations
   - Pairwise becomes combinatorially expensive with multiple configs
   - Easier to do statistical analysis on scalar scores

2. **Your 5-dimension rubric is well-designed**: Accuracy, Completeness, Conciseness, Coherence, Faithfulness aligns with standard evaluation dimensions. Consider adding:
   - **Instruction-following**: Does the output address the actual question?
   - Use 1-5 Likert scale per dimension (not 1-10; LLMs cluster scores more reliably on narrower scales)

3. **Rubric structure**: Provide detailed criteria for each score level per dimension. G-Eval style (chain-of-thought before scoring) improves alignment with humans.

### Calibration Protocol

1. **Create gold standard set**: 30-50 examples scored by human(s)
2. **Run judge on gold set**: Compare LLM scores to human scores
3. **Compute alignment**: Use Krippendorff's alpha (handles ordinal data, missing values) or weighted Cohen's kappa
4. **Iterate on rubric**: Adjust scoring criteria until alpha >= 0.80
5. **Deploy bias-adjusted estimator**: Use the [LLM-judge-reporting framework](https://github.com/UW-Madison-Lee-Lab/LLM-judge-reporting)

### Sample Size for Calibration

Per [Lee & Zeng (2025)](https://arxiv.org/html/2511.21140v1):
- **Calibration set**: ~200 examples per label type for confidence intervals under 0.1 width
- **Adaptive allocation**: Use pilot sample (~10 per type) to estimate error ratio, then allocate remaining budget proportionally
- **Test set**: Can be arbitrarily large since LLM evaluation is cheap
- **Bias-adjusted estimator**: theta_hat = (p_hat + q0 - 1) / (q0 + q1 - 1), where q0 = specificity, q1 = sensitivity

### Practical Tips for Claude as Judge

- Claude tends toward diplomatic/generous scoring -- use strict rubrics with explicit "this score means failure" examples
- Request explanation *before* score (chain-of-thought improves consistency)
- Temperature 0 for reproducibility
- Run each evaluation 3x and take median to reduce stochasticity
- Self-preference: if Claude generates AND judges, expect inflated scores. Mitigate by using Ollama-hosted model for generation.

---

## 3. RAG vs Context Preprocessing

### Key Distinctions

| Dimension | Standard RAG | Context Forge (Preprocessing) | Hybrid |
|-----------|-------------|-------------------------------|--------|
| **When** | Query-time retrieval | Pre-query context preparation | Both |
| **What** | Finds relevant chunks | Restructures/compresses known context | Retrieves then restructures |
| **Model role** | Embedding model + reranker | Small LLM as context engineer | Both |
| **Context source** | Large document corpus | Known, bounded context | Either |
| **Output** | Top-K chunks | Restructured, compressed context | Optimized context window |
| **Quality mechanism** | Retrieval precision/recall | Compression fidelity | Both |

### Where Context Forge Differs

Context Forge is NOT RAG because:
1. The context is already known/selected -- no retrieval step needed
2. The small model's job is *restructuring*, not *finding*
3. The optimization target is context quality, not retrieval relevance
4. It operates on the full context window, not individual chunks

Context Forge IS closer to:
- **RECOMP's abstractive compressor**: Small model summarizing retrieved docs
- **Contextual RAG (Anthropic's approach)**: Adding context to chunks before indexing
- **RAG 2.0's "dynamic context weighting"**: On-the-fly summarization of retrievals

### Hybrid Approaches Worth Considering

1. **Retrieve-then-Compress**: Use RAG for retrieval, then Context Forge for preprocessing before sending to Claude. This is essentially RECOMP's architecture.
2. **Context Forge as RAG Post-processor**: Position Context Forge between retrieval and generation. The [SPLICE method](https://ragflow.io/blog/rag-review-2025-from-rag-to-context) (Semantic Preservation with Length-Informed Chunking) reports 27% improvement in answer precision.
3. **Hierarchical Context Engineering**: Small model creates a "context map" (summary + key facts + metadata) that fits in a fraction of the token budget, with full context available for targeted extraction.

### The "Lost in the Middle" Opportunity

Research consistently shows LLMs lose 15-47% performance as context grows ([Stanford research](https://arxiv.org/html/2501.01880v1)). Context Forge can specifically address this by:
- Putting most important information at beginning/end
- Removing redundant context that dilutes attention
- Restructuring to match the target model's attention patterns

---

## 4. Small Model Orchestration Patterns

### Existing Work

#### FrugalGPT (Stanford, 2023)
- [Paper](https://arxiv.org/abs/2305.05176)
- Three strategies: prompt adaptation, LLM approximation, **LLM cascade**
- Cascade: start with smallest model, escalate if confidence is low
- **98% cost reduction** matching GPT-4 performance, or **4% accuracy improvement** at same cost

#### RouteLLM (LMSYS, ICLR 2025)
- [GitHub](https://github.com/lm-sys/RouteLLM) | [Paper](https://arxiv.org/pdf/2406.18665)
- Routes between strong (expensive) and weak (cheap) models based on query difficulty
- **85% cost reduction on MT Bench**, 45% on MMLU, 35% on GSM8K
- Uses preference data for training the router

#### OrchestraLLM (Dialogue State Tracking)
- Uses SLMs and LLMs with complementary strengths
- **50%+ cost reduction** while enhancing performance vs LLM-only

#### Pick and Spin (Dec 2025)
- [Paper](https://arxiv.org/abs/2512.22402)
- Kubernetes-based multi-model orchestration
- Hybrid routing: keyword heuristics + lightweight DistilBERT classifier
- **21.6% higher success, 30% lower latency, 33% lower GPU cost**

#### xRouter (2025)
- [Paper](https://arxiv.org/html/2510.08439v1)
- RL-trained router that can either respond directly or delegate to external models
- Treats routing as sequential decision-making under economic constraints

#### Router-R1 (2025)
- Uses reinforcement learning for dynamic, multi-round routing
- Can interleave reasoning and model invocation across rounds

### Patterns Relevant to Context Forge

1. **Preprocessor Pattern** (Context Forge's core): Small model restructures input, large model generates output. Similar to RECOMP abstractive compressor, but generalized.

2. **Cascade with Early Exit**: Small model attempts answer; only sends to Claude if confidence is low. Could save API costs for simple queries.

3. **Router + Preprocessor**: Classify query complexity first, then apply appropriate preprocessing level (light/heavy compression, restructuring, or passthrough).

4. **Critic Pattern**: Small model reviews large model output for consistency/completeness, requests regeneration if needed.

### Ollama-Specific Considerations

- Context window: Most Ollama models support 4K-32K tokens; some (Qwen, Llama) support 128K+
- Recommended preprocessing models: Qwen2.5-7B-Instruct (good instruction following), Llama-3.1-8B (general), Phi-3-mini (efficient)
- Set `OLLAMA_NUM_CTX` to match your preprocessing needs
- Latency: Expect 2-10 seconds per preprocessing call on consumer hardware; factor into evaluation pipeline timing

---

## 5. Evaluation Pipeline Design

### Recommended Framework: Promptfoo

For Context Forge's use case, [Promptfoo](https://github.com/promptfoo/promptfoo) is the strongest fit because:
- **YAML-based configuration**: Declarative test definitions
- **Ollama native support**: Direct integration with local models
- **LLM-as-Judge built-in**: Supports custom judge prompts
- **A/B comparison**: Side-by-side configuration comparison
- **CLI + Web UI**: Review results visually
- **No cloud dependency**: Runs entirely locally
- **CI/CD integration**: Can automate in pipeline

```yaml
# Example promptfooconfig.yaml for Context Forge
providers:
  - id: ollama:qwen2.5:7b  # Preprocessing model
  - id: anthropic:claude-sonnet-4-20250514  # Target model

prompts:
  - "{{preprocessed_context}}\n\nQuestion: {{question}}"
  - "{{raw_context}}\n\nQuestion: {{question}}"

tests:
  - vars:
      raw_context: "file://contexts/test1.txt"
      question: "What are the key findings?"
    assert:
      - type: llm-rubric
        value: "Rate accuracy, completeness, conciseness (1-5 each)"
```

### Alternative Frameworks Comparison

| Framework | Best For | Local Support | LLM-Judge | Cost |
|-----------|---------|--------------|-----------|------|
| **Promptfoo** | Config comparison, prompt testing | Excellent (Ollama) | Built-in | Free/OSS |
| **DeepEval** | Python-native eval, CI/CD | Good | G-Eval, custom | Free/OSS |
| **RAGAS** | RAG-specific metrics | Via LangChain | Built-in | Free/OSS |
| **LangSmith** | Observability, production monitoring | Via LangChain | Via LangChain | Paid SaaS |
| **Custom Python** | Full control, research | Full | Custom | Free |

### Recommended Evaluation Architecture for Context Forge

```
Test Cases (YAML/JSON)
    |
    v
Preprocessing Layer (Ollama models, multiple configs)
    |
    v
Target Model (Claude API, fixed config)
    |
    v
Judge Model (Claude API, separate call with rubric)
    |
    v
Score Collection (JSON/CSV per run)
    |
    v
Statistical Analysis (Python: scipy, bootstrap)
    |
    v
Report Generation (comparison tables, confidence intervals)
```

### Metrics Beyond Accuracy

| Metric | What It Measures | Why It Matters |
|--------|-----------------|----------------|
| **Token count reduction** | Raw compression ratio | Cost savings |
| **Latency (preprocessing + generation)** | Total wall-clock time | Usability |
| **Cost per query** | API tokens consumed | Budget impact |
| **Faithfulness** | Are claims grounded in source? | Prevents hallucination from compression |
| **Information retention** | Key facts preserved | Compression quality |
| **Coherence** | Output readability | User experience |
| **Task-specific accuracy** | Correct answers to questions | End-to-end quality |
| **Consistency** | Score variance across runs | Reliability |
| **Context utilization** | Does model use provided context? | Compression effectiveness |

---

## 6. Statistical Methodology

### Recommended Statistical Framework

For Context Forge's configuration comparisons:

#### Primary Test: Paired Bootstrap + Permutation

Per [paired bootstrap protocol (2025)](https://arxiv.org/html/2511.19794v1):

1. **Fix randomness**: Same test cases, same random seeds across configurations
2. **Compute per-case deltas**: For each test case, delta = score(config_A) - score(config_B)
3. **BCa Bootstrap CI**: Bias-corrected and accelerated bootstrap on deltas (10,000+ resamples)
4. **Permutation test**: Sign-flip permutation on per-case deltas
5. **Declare significance only if**: BCa CI lower bound > 0 AND permutation p-value < 0.05

#### Minimum Runs Per Configuration

| Scenario | Runs Needed | Notes |
|----------|------------|-------|
| Detecting large effects (>5 points) | 3-5 runs | Conservative but detectable |
| Detecting moderate effects (2-5 points) | 5-10 runs | Recommended minimum |
| Detecting small effects (<2 points) | 10+ runs | May need 15-20 for power |
| LLM temperature variation | 3-5 runs per config | Takes median |

**Practical recommendation for Context Forge**: 5 runs per configuration, 50+ test cases, paired analysis. This gives reasonable power for the moderate effect sizes (2-5 point improvements) that context preprocessing typically produces.

#### Handling LLM Output Variance

1. **Temperature 0**: Reduces but doesn't eliminate variance (non-determinism in sampling, batching)
2. **Multiple runs**: 3-5 runs per test case, take median score
3. **Paired analysis**: Same test case across configs eliminates inter-case variance
4. **Effect size reporting**: Report Cohen's d or Cliff's delta alongside p-values
5. **IMMBA framework**: Consider Linear Mixed Models + bootstrap for decomposing variance into fixed effects (configuration) and random effects (test case, run)

#### Calibration Dataset Statistical Requirements

Per [Lee & Zeng (2025)](https://arxiv.org/html/2511.21140v1):
- **~200 examples per label type** for CI width < 0.1
- **Pilot sample**: Start with 10 per type, use adaptive allocation
- **Python implementation**: [UW-Madison GitHub](https://github.com/UW-Madison-Lee-Lab/LLM-judge-reporting)

#### Inter-Rater Reliability

For validating Claude-as-Judge against human scores:
- **Krippendorff's alpha**: Best for ordinal data, handles missing values. Target alpha >= 0.80.
- **Weighted Cohen's kappa**: For two-rater agreement. Target >= 0.60 (moderate agreement minimum).
- **Spearman's rho**: For rank correlation between judge and human scores.

### Practical Statistical Workflow

```python
# Pseudocode for Context Forge evaluation
from scipy.stats import wilcoxon, bootstrap
import numpy as np

# 1. Collect paired scores
baseline_scores = [...]   # raw context -> Claude scores
forge_scores = [...]       # preprocessed context -> Claude scores

# 2. Compute deltas (paired)
deltas = np.array(forge_scores) - np.array(baseline_scores)

# 3. Wilcoxon signed-rank (non-parametric paired test)
stat, p_value = wilcoxon(deltas, alternative='greater')

# 4. Bootstrap CI on deltas
boot_result = bootstrap((deltas,), np.mean, n_resamples=10000,
                         method='BCa', confidence_level=0.95)

# 5. Effect size (Cliff's delta for ordinal data)
def cliffs_delta(x, y):
    n = len(x)
    count = sum(1 for i in x for j in y if i > j) - \
            sum(1 for i in x for j in y if i < j)
    return count / (n * n)

# 6. Report: mean delta, CI, p-value, effect size
```

---

## 7. Token Compression Research -- Ratios and Cliffs

### Empirical Compression Results Summary

| Method | Compression Ratio | Quality Retention | Task Type |
|--------|------------------|-------------------|-----------|
| LLMLingua | Up to 20x | Minimal loss | ICL, reasoning |
| LLMLingua-2 | 5-10x | 95-98% accuracy | General NLP |
| LongLLMLingua | 4x | +21.4% on NaturalQuestions | Long-context QA |
| RECOMP (extractive) | 6% retention (16x) | Minimal loss | QA, language modeling |
| RECOMP (abstractive) | 6% retention (16x) | Near-baseline | QA |
| Selective Context | 2x (50% removal) | Minor BERTScore drop | General |
| CompactPrompt | 2-2.5x | <5% drop, often improvement | Financial QA |
| Reranker-based extractive | 3-8x (safe), 10x+ | Best across 9 datasets | QA, summarization |

### The Degradation Cliff

Based on [Characterizing Prompt Compression (2024)](https://arxiv.org/html/2407.08892v1):

**General pattern**:
- **2-5x compression**: Safe zone. Most methods retain 95%+ quality.
- **5-10x compression**: Method-dependent. Extractive (sentence-level) outperforms token pruning. Quality retention 85-95%.
- **10-20x compression**: Only the best methods (LLMLingua, RECOMP) survive. Content type matters enormously.
- **>20x compression**: Consistent degradation across all methods.

**The cliff is task-dependent, not ratio-dependent**:
- Reasoning tasks: Cliff at ~5x (need full chain of logic)
- Factual QA: Can tolerate 10-15x (key facts are sparse)
- Summarization: Tolerates 5-10x (already lossy by nature)
- Code generation: Cliff at 2-3x (syntax is dense)
- Mathematical reasoning: Cliff at 2-3x (every symbol matters)

**Critical finding**: [CompactPrompt results](https://arxiv.org/html/2510.18043v1) show that **quality can actually improve** with moderate compression (2-2.5x) because:
1. Removes distracting/irrelevant context
2. Reduces "Lost in the Middle" effect
3. Forces focus on essential information

### Implications for Context Forge

- **Target 2-5x compression** as the default operating range
- **Measure quality at multiple compression levels** (1x, 2x, 3x, 5x, 10x) to find the cliff per content type
- **Content-type-aware preprocessing**: Apply different strategies to code vs prose vs tables
- **Quality improvement zone**: The 2-3x range may actually produce *better* outputs than raw context -- this is a key hypothesis to test

---

## Best Practices Summary

### For Context Forge Implementation

1. **Start with extractive compression** (sentence selection) before attempting abstractive (restructuring). Extractive is simpler, more faithful, and establishes a baseline.

2. **Use Qwen2.5-7B or Llama-3.1-8B via Ollama** as preprocessing models. Both have strong instruction-following and fit in consumer GPU memory.

3. **Evaluation pipeline**: Promptfoo for configuration management + custom Python for statistical analysis. Don't try to build everything from scratch.

4. **Judge configuration**: Claude with 5-dimension rubric (1-5 Likert), chain-of-thought before scoring, temperature 0, 3 runs per judgment taking median.

5. **Statistical rigor**: 5 paired runs per configuration minimum, 50+ test cases, paired bootstrap with BCa CI, Wilcoxon signed-rank test, Krippendorff's alpha >= 0.80 against human gold standard.

6. **Calibration**: 30-50 human-scored examples for gold standard, 200 per label type for judge calibration.

7. **Report compression ratio AND quality jointly** -- a Pareto frontier of ratio vs quality is more informative than any single number.

---

## Common Pitfalls to Avoid

1. **Evaluating compression ratio alone**: High compression is meaningless if quality drops. Always report the Pareto curve.
2. **Single-run comparisons**: LLM outputs are stochastic. Single comparisons are unreliable.
3. **Using the same model as generator and judge**: Self-preference bias inflates scores. Use different model families.
4. **Ignoring content type**: Compression effectiveness varies wildly by content type. Don't use a single compression strategy.
5. **Over-optimizing on benchmarks**: Real-world context is messier than academic datasets. Include diverse, realistic test cases.
6. **Assuming compression always helps**: Sometimes the full context IS necessary. Measure the failure modes.
7. **Ignoring latency**: If preprocessing adds 5 seconds but saves 2 seconds of API inference, that's a net loss for interactive use cases. May still win for batch processing.

---

## Knowledge Gaps

1. **Few studies on using Ollama-sized models (7-8B) specifically as context preprocessors** for API models. Most research uses GPT-4 or larger models. The quality of 7B model preprocessing for Claude specifically is untested in the literature.

2. **Limited research on restructuring (vs pure compression)**. Most work focuses on removing tokens, not reorganizing them. Context Forge's restructuring hypothesis is novel.

3. **No standard benchmark for "context preprocessing quality"**. You'll need to create evaluation datasets.

4. **Inter-model transfer of preprocessing**: Does preprocessing optimized for GPT-4 transfer to Claude? RECOMP suggests partial transfer, but systematic study is lacking.

5. **Long-term drift**: As target models update, does preprocessing quality degrade? No longitudinal studies exist.

---

## Recommendations for Context Forge

### Phase 1: Baseline and Measurement (Week 1-2)
- Set up Promptfoo with Ollama + Claude API
- Create 50+ diverse test cases (mix of content types)
- Measure raw (no preprocessing) baseline across all dimensions
- Score 30-50 examples manually for gold standard

### Phase 2: Extractive Compression (Week 3-4)
- Implement sentence-level extraction using Ollama model
- Test at 2x, 3x, 5x compression ratios
- Compare against LLMLingua-2 as baseline (pip install llmlingua)
- Statistical analysis: paired bootstrap on quality deltas

### Phase 3: Abstractive Restructuring (Week 5-6)
- Implement context restructuring prompts for Ollama
- Test "summarize and reorganize" vs "extract key facts" vs "create context map"
- Measure quality improvement zone (where compression helps)

### Phase 4: Optimization (Week 7-8)
- Content-type-aware routing (different strategies for different inputs)
- Hybrid approaches (extract + restructure)
- Pareto frontier analysis
- Statistical significance confirmation

### Tools to Install

```bash
# Evaluation framework
npm install -g promptfoo

# Context compression baseline
pip install llmlingua

# Statistical analysis
pip install scipy numpy pandas matplotlib

# LLM-as-Judge calibration
pip install krippendorff  # inter-rater reliability

# Evaluation framework (alternative)
pip install deepeval

# RAG evaluation metrics
pip install ragas
```

---

## Sources

### Core Papers

1. [LLMLingua (EMNLP'23)](https://arxiv.org/abs/2310.05736) - Microsoft, prompt compression via perplexity-based pruning
2. [LLMLingua-2 (ACL'24)](https://llmlingua.com/llmlingua2.html) - Token classification approach
3. [LongLLMLingua (ACL'24)](https://aclanthology.org/2024.acl-long.91/) - Long-context compression
4. [RECOMP (ICLR'24)](https://arxiv.org/abs/2310.04408) - Extractive and abstractive context compression
5. [Selective Context (EMNLP'23)](https://aclanthology.org/2023.emnlp-main.391/) - Self-information token pruning
6. [Prompt Compression Survey (NAACL'25)](https://aclanthology.org/2025.naacl-long.368/) - Comprehensive taxonomy
7. [CompactPrompt (Oct 2025)](https://arxiv.org/abs/2510.18043) - Unified compression pipeline
8. [Characterizing Prompt Compression (2024)](https://arxiv.org/html/2407.08892v1) - Benchmark comparison
9. [DisComp (NAACL Findings'25)](https://aclanthology.org/2025.findings-naacl.58/) - Two-stage optimization

### LLM-as-Judge

10. [A Survey on LLM-as-a-Judge (Nov 2024)](https://arxiv.org/abs/2411.15594) - Comprehensive survey
11. [How to Correctly Report LLM-as-a-Judge (Nov 2025)](https://arxiv.org/html/2511.21140v1) - Statistical methodology
12. [Self-Preference Bias (Oct 2024)](https://arxiv.org/html/2410.21819v2) - Bias quantification
13. [Justice or Prejudice? (2024)](https://llm-judge-bias.github.io/) - Bias benchmarking
14. [LLMs-as-Judges Comprehensive Survey (Dec 2024)](https://arxiv.org/html/2412.05579v2)
15. [Evidently AI Guide](https://www.evidentlyai.com/llm-guide/llm-as-a-judge) - Practical implementation

### Routing and Orchestration

16. [FrugalGPT (2023)](https://arxiv.org/abs/2305.05176) - LLM cascade cost optimization
17. [RouteLLM (ICLR'25)](https://github.com/lm-sys/RouteLLM) - Preference-based routing
18. [Pick and Spin (Dec 2025)](https://arxiv.org/abs/2512.22402) - Multi-model orchestration
19. [xRouter (2025)](https://arxiv.org/html/2510.08439v1) - RL-trained routing

### Statistical Methods

20. [Paired Bootstrap Protocol (Nov 2025)](https://arxiv.org/html/2511.19794v1) - Statistical significance for small improvements
21. [IMMBA (2025)](https://www.scitepress.org/Papers/2025/138194/138194.pdf) - Mixed models + bootstrap for LLM evaluation
22. [LLM-judge-reporting GitHub](https://github.com/UW-Madison-Lee-Lab/LLM-judge-reporting) - Python implementation

### RAG and Context Engineering

23. [Long Context vs RAG (Jan 2025)](https://arxiv.org/html/2501.01880v1) - Evaluation and revisits
24. [From RAG to Context (2025)](https://ragflow.io/blog/rag-review-2025-from-rag-to-context) - Field evolution review
25. [Context Engineering Survey (Jul 2025)](https://arxiv.org/abs/2507.13334) - Comprehensive survey
26. [LangChain Context Engineering Blog](https://blog.langchain.com/context-engineering-for-agents/)

### Evaluation Frameworks

27. [Promptfoo](https://github.com/promptfoo/promptfoo) - OSS evaluation toolkit
28. [DeepEval](https://github.com/confident-ai/deepeval) - Python LLM evaluation
29. [RAGAS](https://docs.ragas.io/en/stable/) - RAG evaluation metrics

---

## Related Topics for Follow-Up Research

- **Attention pattern analysis**: How do different models attend to compressed vs raw context?
- **Multi-turn context management**: How does preprocessing interact with conversation history?
- **Agentic context**: How should tool outputs and agent traces be preprocessed?
- **Cost modeling**: Building accurate cost models for preprocessing + API calls
- **Prompt caching interaction**: How does Claude's prompt caching interact with preprocessed context?
- **Evaluation dataset design**: What makes a good test case for context preprocessing?
