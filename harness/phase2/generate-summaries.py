#!/usr/bin/env python3
"""
Phase 2.2: Generate I4 summary tables using different models.

Creates strategy folders for each model variant:
  strategies/<dataset>/I4-<model>/
    ├── data -> symlink
    ├── summaries.md (generated by the specified model)
    └── CLAUDE.md (@summaries.md)

Supported backends:
  - ollama: qwen2.5:32b, qwen2.5:7b-instruct, llama3.2:3b, etc.
  - anthropic: haiku, sonnet (requires ANTHROPIC_API_KEY)
  - openai: gpt-4o-mini (requires OPENAI_API_KEY)
  - gemini: gemini-2.0-flash (requires GEMINI_API_KEY)
  - template: heuristic (no LLM, uses filename + keywords)

Usage:
    python3 generate-summaries.py --model ollama:qwen2.5:32b
    python3 generate-summaries.py --model ollama:qwen2.5:7b-instruct
    python3 generate-summaries.py --model anthropic:sonnet
    python3 generate-summaries.py --model gemini:flash
    python3 generate-summaries.py --model template
    python3 generate-summaries.py --list          # Show available models
    python3 generate-summaries.py --all-ollama    # Run all Ollama models
"""

import argparse
import json
import os
import re
import sys
import time
from pathlib import Path

PROJECT_ROOT = Path(__file__).resolve().parent.parent.parent

# Auto-load .env if present
_env_file = PROJECT_ROOT / ".env"
if _env_file.exists():
    for line in _env_file.read_text().splitlines():
        line = line.strip()
        if line and not line.startswith("#") and "=" in line:
            key, _, value = line.partition("=")
            os.environ.setdefault(key.strip(), value.strip())

STRATEGIES_DIR = PROJECT_ROOT / "strategies"
SOONG_SOURCE = PROJECT_ROOT / "soong-daystrom" / "_source-v5"
OBSIDIAN_SOURCE = PROJECT_ROOT / "test-datasets" / "obsidian"

DATASETS = {
    "soong-v5": SOONG_SOURCE,
    "obsidian": OBSIDIAN_SOURCE,
}

# Model registry: name -> (backend, model_id, strategy_suffix)
MODELS = {
    "template":                ("template",   None,                      "I4-template"),
    "ollama:qwen2.5:32b":     ("ollama",     "qwen2.5:32b",            "I4-qwen32b"),
    "ollama:qwen2.5:7b-instruct": ("ollama", "qwen2.5:7b-instruct",   "I4-qwen7b"),
    "ollama:llama3.2:3b":     ("ollama",     "llama3.2:3b",            "I4-llama3b"),
    "anthropic:haiku":        ("anthropic",  "claude-haiku-4-5-20251001", "I4-haiku"),
    "anthropic:sonnet":       ("anthropic",  "claude-4-sonnet-20250514", "I4-sonnet"),
    "cli:sonnet":             ("cli",        "sonnet",                  "I4-sonnet"),
    "openai:gpt-4o-mini":     ("openai",     "gpt-4o-mini",            "I4-gpt4omini"),
    "gemini:flash":           ("gemini",     "gemini-2.0-flash",       "I4-geminiflash"),
}

OLLAMA_URL = os.environ.get("OLLAMA_URL", "http://localhost:11434")

# ─── Summarization backends ─────────────────────────────────────────

def summarize_template(file_path: str, text: str, keywords: list[str], dataset: str) -> str:
    """Heuristic summary from filename + keywords (no LLM)."""
    parts = Path(file_path.replace("data/", "")).parts
    directory = parts[0] if len(parts) > 1 else ""
    filename = Path(parts[-1]).stem.replace("-", " ").replace("_", " ")
    filename = re.sub(r'^\d{4}[\.\-]\d{2}[\.\-]\d{2}[\s_\-]*', '', filename).strip()
    kw = ", ".join(keywords[:4])
    dir_label = directory.replace("-", " ").title()

    # Extract first heading
    heading_match = re.search(r'^#+ (.+)$', text, re.MULTILINE)
    title = heading_match.group(1).strip() if heading_match and len(heading_match.group(1)) > 10 else filename

    return f"{dir_label} documentation about {title}, covering {kw}."


def summarize_ollama(text: str, model_id: str) -> str:
    """Generate summary via Ollama API."""
    import urllib.request

    # Truncate to ~4000 words to fit smaller model context windows
    words = text.split()
    if len(words) > 4000:
        text = " ".join(words[:4000])

    prompt = (
        "Summarize the following document in exactly one sentence. "
        "Focus on what the document is about, who/what it covers, and key facts. "
        "Be specific — include names, numbers, and concrete details. "
        "Return ONLY the summary sentence, no preamble.\n\n"
        f"{text}"
    )

    payload = json.dumps({
        "model": model_id,
        "prompt": prompt,
        "stream": False,
        "options": {"temperature": 0.1, "num_predict": 150},
    }).encode()

    req = urllib.request.Request(
        f"{OLLAMA_URL}/api/generate",
        data=payload,
        headers={"Content-Type": "application/json"},
    )

    try:
        with urllib.request.urlopen(req, timeout=300) as resp:
            result = json.loads(resp.read())
            return result.get("response", "").strip().split("\n")[0]  # First line only
    except Exception as e:
        return f"[Error: {e}]"


def summarize_anthropic(text: str, model_id: str) -> str:
    """Generate summary via Anthropic API."""
    api_key = os.environ.get("ANTHROPIC_API_KEY")
    if not api_key:
        raise RuntimeError("ANTHROPIC_API_KEY not set")

    import urllib.request

    # Truncate to ~8000 words
    words = text.split()
    if len(words) > 8000:
        text = " ".join(words[:8000])

    payload = json.dumps({
        "model": model_id,
        "max_tokens": 150,
        "messages": [{
            "role": "user",
            "content": (
                "Summarize the following document in exactly one sentence. "
                "Focus on what it's about, key entities, and concrete details. "
                "Return ONLY the sentence.\n\n" + text
            ),
        }],
    }).encode()

    req = urllib.request.Request(
        "https://api.anthropic.com/v1/messages",
        data=payload,
        headers={
            "Content-Type": "application/json",
            "x-api-key": api_key,
            "anthropic-version": "2023-06-01",
        },
    )

    try:
        with urllib.request.urlopen(req, timeout=30) as resp:
            result = json.loads(resp.read())
            return result["content"][0]["text"].strip().split("\n")[0]
    except Exception as e:
        return f"[Error: {e}]"


def summarize_cli(text: str, model_id: str) -> str:
    """Generate summary via claude -p CLI (uses Claude Code subscription)."""
    import subprocess

    # Truncate to ~8000 words
    words = text.split()
    if len(words) > 8000:
        text = " ".join(words[:8000])

    prompt = (
        "Summarize the following document in exactly one sentence. "
        "Focus on what it's about, key entities, and concrete details. "
        "Return ONLY the summary sentence, nothing else.\n\n" + text
    )

    try:
        env = os.environ.copy()
        env.pop("CLAUDECODE", None)  # Allow nested invocation
        result = subprocess.run(
            ["claude", "-p", "--model", model_id, "--output-format", "text",
             "--max-turns", "1"],
            input=prompt,
            capture_output=True,
            text=True,
            timeout=60,
            env=env,
        )

        if result.returncode != 0:
            return f"[Error: exit {result.returncode} — {result.stderr[:100]}]"

        return result.stdout.strip().split("\n")[0]
    except subprocess.TimeoutExpired:
        return "[Error: timeout after 60s]"
    except Exception as e:
        return f"[Error: {e}]"


def summarize_gemini(text: str, model_id: str) -> str:
    """Generate summary via Google Gemini API with retry on rate limits."""
    api_key = os.environ.get("GEMINI_API_KEY")
    if not api_key:
        raise RuntimeError("GEMINI_API_KEY not set")

    import urllib.request
    import urllib.error

    # Gemini has large context, but truncate to keep costs reasonable
    words = text.split()
    if len(words) > 8000:
        text = " ".join(words[:8000])

    payload = json.dumps({
        "contents": [{
            "parts": [{
                "text": (
                    "Summarize the following document in exactly one sentence. "
                    "Focus on what it's about, key entities, and concrete details. "
                    "Return ONLY the sentence.\n\n" + text
                ),
            }],
        }],
        "generationConfig": {
            "maxOutputTokens": 150,
            "temperature": 0.1,
        },
    }).encode()

    # Retry with exponential backoff on 429
    max_retries = 4
    for attempt in range(max_retries):
        req = urllib.request.Request(
            f"https://generativelanguage.googleapis.com/v1beta/models/{model_id}:generateContent?key={api_key}",
            data=payload,
            headers={"Content-Type": "application/json"},
        )
        try:
            with urllib.request.urlopen(req, timeout=30) as resp:
                result = json.loads(resp.read())
                return result["candidates"][0]["content"]["parts"][0]["text"].strip().split("\n")[0]
        except urllib.error.HTTPError as e:
            if e.code == 429 and attempt < max_retries - 1:
                wait = 15 * (2 ** attempt)  # 15s, 30s, 60s, 120s
                print(f"      Rate limited, waiting {wait}s (attempt {attempt + 1}/{max_retries})...")
                time.sleep(wait)
                continue
            return f"[Error: HTTP {e.code}]"
        except Exception as e:
            return f"[Error: {e}]"

    return "[Error: max retries exceeded]"


def summarize_openai(text: str, model_id: str) -> str:
    """Generate summary via OpenAI API."""
    api_key = os.environ.get("OPENAI_API_KEY")
    if not api_key:
        raise RuntimeError("OPENAI_API_KEY not set")

    import urllib.request

    words = text.split()
    if len(words) > 8000:
        text = " ".join(words[:8000])

    payload = json.dumps({
        "model": model_id,
        "messages": [
            {"role": "system", "content": "Summarize documents in exactly one sentence. Be specific with names and numbers."},
            {"role": "user", "content": text},
        ],
        "max_tokens": 150,
        "temperature": 0.1,
    }).encode()

    req = urllib.request.Request(
        "https://api.openai.com/v1/chat/completions",
        data=payload,
        headers={
            "Content-Type": "application/json",
            "Authorization": f"Bearer {api_key}",
        },
    )

    try:
        with urllib.request.urlopen(req, timeout=30) as resp:
            result = json.loads(resp.read())
            return result["choices"][0]["message"]["content"].strip().split("\n")[0]
    except Exception as e:
        return f"[Error: {e}]"


# ─── Keyword index parser (for template backend) ────────────────────

def parse_keyword_index(dataset: str) -> dict[str, list[str]]:
    """Parse I1 keyword index into {file_path: [keywords]}."""
    index_path = STRATEGIES_DIR / dataset / "I1" / "index.md"
    result = {}
    if index_path.exists():
        for line in index_path.read_text().split("\n"):
            match = re.match(r'\| `([^`]+)` \| (.+) \|', line)
            if match:
                result[match.group(1)] = [k.strip() for k in match.group(2).split(",")]
    return result


# ─── Main builder ────────────────────────────────────────────────────

def get_md_files(source_dir: Path) -> list[Path]:
    files = sorted(source_dir.rglob("*.md"))
    return [f for f in files if f.name != "questions.json"]


def _parse_existing_summaries(summary_path: Path) -> dict[str, str]:
    """Parse existing summaries.md into {file_path: summary} dict."""
    existing = {}
    if summary_path.exists():
        for line in summary_path.read_text().split("\n"):
            match = re.match(r'\| `([^`]+)` \| (.+) \|$', line)
            if match:
                existing[match.group(1)] = match.group(2)
    return existing


def build_i4_variant(model_name: str, dataset: str, source_dir: Path,
                     retry_failures: bool = False):
    """Build an I4 variant strategy folder with summaries from a specific model."""
    backend, model_id, strategy_name = MODELS[model_name]
    strategy_dir = STRATEGIES_DIR / dataset / strategy_name

    strategy_dir.mkdir(parents=True, exist_ok=True)

    # Create data symlink
    data_link = strategy_dir / "data"
    if data_link.exists() or data_link.is_symlink():
        data_link.unlink()
    if dataset == "soong-v5":
        data_link.symlink_to("../../../soong-daystrom/_source-v5")
    else:
        data_link.symlink_to("../../../test-datasets/obsidian")

    files = get_md_files(source_dir)
    keyword_index = parse_keyword_index(dataset) if backend == "template" else {}

    # Load existing summaries for retry mode
    existing = {}
    if retry_failures:
        existing = _parse_existing_summaries(strategy_dir / "summaries.md")
        failures = sum(1 for v in existing.values() if v.startswith("Document at "))
        total = len(existing)
        print(f"  Retry mode: {failures} failures out of {total} entries")
        if failures == 0:
            print(f"  No failures to retry, skipping.")
            return

    lines = ["# Summary Index\n"]
    lines.append(f"Each file with a one-sentence summary (generated by {model_name}).\n")
    lines.append("| File | Summary |")
    lines.append("|------|---------|")

    errors = 0
    retried = 0
    skipped = 0
    start = time.time()

    for i, f in enumerate(files):
        rel = f.relative_to(source_dir)
        file_path = f"data/{rel}"

        # In retry mode, keep existing good summaries
        if retry_failures and file_path in existing:
            old_summary = existing[file_path]
            if not old_summary.startswith("Document at "):
                # Good summary, keep it
                lines.append(f"| `{file_path}` | {old_summary} |")
                skipped += 1
                continue
            else:
                retried += 1
                print(f"    Retrying: {file_path}")

        text = f.read_text(errors='replace')

        if backend == "template":
            keywords = keyword_index.get(file_path, [])
            summary = summarize_template(file_path, text, keywords, dataset)
        elif backend == "ollama":
            summary = summarize_ollama(text, model_id)
            time.sleep(0.1)  # Light rate limiting for local
        elif backend == "anthropic":
            summary = summarize_anthropic(text, model_id)
            time.sleep(0.5)  # API rate limiting
        elif backend == "cli":
            summary = summarize_cli(text, model_id)
            time.sleep(1.0)  # Generous delay between CLI calls
        elif backend == "gemini":
            summary = summarize_gemini(text, model_id)
            time.sleep(8.0)  # Free tier: 15 RPM, run at ~7.5 RPM for safety
        elif backend == "openai":
            summary = summarize_openai(text, model_id)
            time.sleep(0.3)

        if summary.startswith("[Error"):
            errors += 1
            summary = f"Document at {file_path}"

        summary = summary.replace("|", "\\|").replace("\n", " ")
        lines.append(f"| `{file_path}` | {summary} |")

        if (i + 1) % 20 == 0 or (retry_failures and retried % 10 == 0):
            elapsed = time.time() - start
            done = (i + 1) - skipped
            if done > 0:
                rate = done / elapsed
                remaining_count = len(files) - i - 1
                # In retry mode, estimate based on failure rate
                if retry_failures:
                    remaining_retries = sum(1 for fp, s in existing.items()
                                            if s.startswith("Document at ") and fp > file_path)
                    remaining_count = remaining_retries
                remaining_time = remaining_count / rate if rate > 0 else 0
                print(f"    {i+1}/{len(files)} ({rate:.1f} files/s, ~{remaining_time:.0f}s remaining)")

    (strategy_dir / "summaries.md").write_text("\n".join(lines) + "\n")

    # CLAUDE.md
    claude_lines = [
        "# Knowledge Base with Summary Index\n",
        "Use the summary index to identify relevant files, then read them for details.\n",
        "@summaries.md",
    ]
    (strategy_dir / "CLAUDE.md").write_text("\n".join(claude_lines) + "\n")

    elapsed = time.time() - start
    size = (strategy_dir / "summaries.md").stat().st_size
    retry_info = f", {retried} retried, {skipped} kept" if retry_failures else ""
    print(f"  BUILT {strategy_name}/{dataset} — {len(files)} summaries, {size:,} bytes, {elapsed:.1f}s, {errors} errors{retry_info}")


def build_i4_grep(dataset: str, source_dir: Path):
    """Build the I4-grep variant: summaries exist but are NOT @-loaded."""
    strategy_dir = STRATEGIES_DIR / dataset / "I4-grep"
    strategy_dir.mkdir(parents=True, exist_ok=True)

    # Symlink
    data_link = strategy_dir / "data"
    if data_link.exists() or data_link.is_symlink():
        data_link.unlink()
    if dataset == "soong-v5":
        data_link.symlink_to("../../../soong-daystrom/_source-v5")
    else:
        data_link.symlink_to("../../../test-datasets/obsidian")

    # Copy summaries.md from the best available I4 variant (or current I4)
    source_summaries = STRATEGIES_DIR / dataset / "I4" / "summaries.md"
    if source_summaries.exists():
        import shutil
        shutil.copy2(source_summaries, strategy_dir / "summaries.md")

    # CLAUDE.md — NO @ref, just instructions
    claude_lines = [
        "# Knowledge Base\n",
        "This directory contains a knowledge base of files in the `data/` folder.\n",
        "A summary index is available at `summaries.md` in this directory.",
        "Use Grep or Read on `summaries.md` to find relevant files by topic,",
        "then Read those data files to answer questions.\n",
        "Do NOT try to load the entire summaries file into context.",
        "Instead, use targeted Grep searches on summaries.md to find the files you need.",
    ]
    (strategy_dir / "CLAUDE.md").write_text("\n".join(claude_lines) + "\n")

    print(f"  BUILT I4-grep/{dataset} — summaries NOT loaded, grep instructions only")


# ─── CLI ─────────────────────────────────────────────────────────────

def main():
    parser = argparse.ArgumentParser(description="Generate I4 summary variants")
    parser.add_argument("--model", help="Model to use (e.g., ollama:qwen2.5:32b)")
    parser.add_argument("--dataset", help="Single dataset (soong-v5 or obsidian)")
    parser.add_argument("--list", action="store_true", help="List available models")
    parser.add_argument("--all-ollama", action="store_true", help="Run all Ollama models")
    parser.add_argument("--grep", action="store_true", help="Build I4-grep variant")
    parser.add_argument("--retry-failures", action="store_true",
                        help="Only regenerate 'Document at...' fallback entries")
    args = parser.parse_args()

    if args.list:
        print("\nAvailable models:")
        print(f"{'Name':<35} {'Backend':<12} {'Strategy folder'}")
        print("-" * 65)
        for name, (backend, model_id, suffix) in MODELS.items():
            print(f"{name:<35} {backend:<12} {suffix}")
        print(f"\n{'I4-grep':<35} {'n/a':<12} I4-grep")
        return

    datasets = {args.dataset: DATASETS[args.dataset]} if args.dataset else DATASETS

    if args.grep:
        print("\n=== Building I4-grep variants ===")
        for dataset, source_dir in datasets.items():
            build_i4_grep(dataset, source_dir)
        return

    if args.all_ollama:
        models = [m for m, (b, _, _) in MODELS.items() if b == "ollama"]
    elif args.model:
        if args.model not in MODELS:
            print(f"Unknown model: {args.model}")
            print(f"Available: {', '.join(MODELS.keys())}")
            sys.exit(1)
        models = [args.model]
    else:
        print("Specify --model, --all-ollama, --grep, or --list")
        sys.exit(1)

    for model_name in models:
        backend = MODELS[model_name][0]
        print(f"\n=== Generating summaries with {model_name} ===")

        # Check prerequisites
        if backend == "anthropic" and not os.environ.get("ANTHROPIC_API_KEY"):
            print(f"  SKIP {model_name} — ANTHROPIC_API_KEY not set")
            continue
        if backend == "openai" and not os.environ.get("OPENAI_API_KEY"):
            print(f"  SKIP {model_name} — OPENAI_API_KEY not set")
            continue
        if backend == "gemini" and not os.environ.get("GEMINI_API_KEY"):
            print(f"  SKIP {model_name} — GEMINI_API_KEY not set")
            continue

        for dataset, source_dir in datasets.items():
            print(f"\n--- {dataset} ---")
            build_i4_variant(model_name, dataset, source_dir,
                             retry_failures=args.retry_failures)

    print("\n=== Done! Run tests with: bash harness/phase2/run-tests.sh --strategy I4-<model> --resume ===")


if __name__ == "__main__":
    main()
